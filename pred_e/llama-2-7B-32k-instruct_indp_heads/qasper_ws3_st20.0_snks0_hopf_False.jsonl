{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney (Section SECREF4)\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Affective Text dataset (Section SECREF3)\n\nQuestion: What is the name of the model they used?\n\nAnswer: B-M (Section SECREF4)\n\nQuestion: What is the", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "linear contiguous series of posts starting with the thread's original post (Document[12]):\"We only consider linear contiguous series of posts starting with the thread's original post to constitute to a context; e.g., INLINEFORM0 . This is a reasonable as MOOC forum posts always reply to the original post or to a subsequent post, which in turn replies to the original post . This is in contrast to forums such as Reddit that have a tree or graph - like structure that require forum structure to be modelled explicitly, such as in BIBREF5 .\"\n", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-score (Document[22]):\"We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall . To see if the improvement obtained is statistically significant over the state - of - the art system with textual sarcasm features alone, we perform McNemar test . The output of the SVM classifier using only linguistic features used for sarcasm detection by joshi2015harnessing and the output of the MILR classifier with the complete set of features", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "plain stacked LSTMs (Document[36]):\"In this subsection, we see the influence of each component of a model on performance by removing or replacing its components . the SNLI dataset is used for experiments, and the best performing configuration is used as a baseline for modifications . We consider the following variants: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, and (iv) models that integrate lower contexts via peephole connections.\"\n\nQuestion: What is the proposed", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "unanswerable (Document[10]):\"jiant is written in Python and uses PyTorch as its backend for computation . It is compatible with models implemented in any programming language that can be wrapped in PyTorch, including TensorFlow, Caffe, and Theano . It is also compatible with models implemented in other languages, such as C++ and Julia, as long as they can be wrapped in PyTorch.\"\n\nQuestion: What is the purpose of the jiant project?\n\nAnswer: to provide a configuration - driven interface for defining transfer learning and representation learning experiments using a bank of over", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "yes (Document[10]):\"The intended audience for privacy policies consists of the general public . This informs the decision to elicit questions from crowdworkers on the contents of privacy policies . We choose not to show the contents of privacy policies to crowdworkers, a procedure motivated by a desire to avoid inadvertent biases BIBREF43, BIBREF44, BIBREF45, BIBREF46, BIBREF47, and encourage crowdworkers to ask a variety of questions beyond only asking questions based on practices described in the document", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "unanswerable\n\nQuestion: Does this method help in fine-grained NER task improvement?\n\nAnswer: unanswerable\n\nQuestion: Does this method help in coarse NER task improvement?\n\nAnswer: unanswerable\n\nQuestion: Does this method help in constituency parsing task improvement?\n\nAnswer: unanswerable\n\nQuestion: Does this method help in named entity recognition task improvement?\n\nAnswer: unanswerable\n\nQuestion: Does this method help in phrase-structure parsing task improvement?\n\nAnswer: unanswerable\n\nQuestion: Does this method help in event", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon reviews BIBREF23, BIBREF24, Yelp restaurant reviews dataset extracted from Yelp Dataset Challenge BIBREF25, and restaurant reviews dataset as part of a Kaggle competition BIBREF26 (TABREF7).\n\nQuestion: What is the approach used in the paper?\n\nAnswer: a deep learning approach that does not require any dictionaries (TABREF1).\n\nQuestion: What is the contribution of the paper?\n\nAnswer: a robust approach that utilizes machine translation to reuse a model trained on one language in other languages (T", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "12.27% (Document[12]):\"To further compare our proposed topic - attention model and the base model, we report an average of area under the curve(AUC) score among all 30 terms: the base model has an average AUC of 0.7189, and our topic - attention model (ELMo+Topic) achieves an average AUC of 0.8196 . We provide a case study in Appendix SECREF9 . The results show that the model can benefit from ELMo, which considers contextual features, and", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "transcribed text (Document[100]):\"We evaluate our model using the Interactive Emotional Dyadic Motion Capture (IEMOCAP) BIBREF18 dataset . This dataset was collected following theatrical theory in order to simulate natural dyadic interactions between actors . We use categorical evaluations with majority agreement . We use only four emotional categories happy, sad, angry, and neutral to compare the performance of our model with other research using the same categories . The IEMOCAP dataset includes five sessions, and each session contains utterances from two speakers (one male and one female", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUTO (Document[19]):\"We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms . Carrot2 Lingo discovered 167 clusters and also assigned labels to these clusters . We then generated 167 clusters using CLUTO as well . CLUTO does not generate cluster labels automatically, hence we used 5 most frequent words within the cluster as its labels.\"\n\nQuestion: What is the purpose of the sentence classification?\n\nAnswer", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT (Bidirectional Encoder Representations from Transformers) (Document[51]):\"We describe baselines on this task, including a human performance baseline . No - Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally - sound way on the basis of information present in the privacy policy . We establish a simple baseline to quantify the effect of identifying every question as unanswerable . Word Count Baseline : To quantify the effect of using simple lexical matching to answer the questions, we retrieve the top candidate policy sentences for each", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "pivoting (BIBREF4, BIBREF5)\n\nQuestion: what are the transfer learning baselines?\n\nAnswer: cross-lingual transfer without pretraining (BIBREF16)\n\nQuestion: what are the multilingual NMT baselines?\n\nAnswer: MNMT (BIBREF19)\n\nQuestion: what are the unsupervised NMT baselines?\n\nAnswer: UNMT (BIBREF26)\n\nQuestion: what is the proposed approach?\n\nAnswer: cross-lingual pretraining based transfer (BIBREF2", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "EmotionLines BIBREF6 (Document[10]):\"EmotionLines BIBREF6 is a dialogue dataset composed of two subsets, Friends and EmotionPush, according to the source of the dialogues . The former comes from the scripts of the Friends TV sitcom . The other is made up of Facebook messenger chats . Each subset includes $1,000$ English dialogues, and each dialogue can be further divided into a few consecutive utterances . All the utterances are annotated by five annotators on a crowd - sourcing platform (Amazon Mechanical", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "several evaluation protocols for each part of the database (Document[26]):\"In addition to speaker verification experiments, we also provided several evaluation protocols for each part of the database . The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance . We also provide two test sets for speech recognition: One normal test set with a few minutes of speech for each speaker and one large test set with more (30 minutes on average) speech that can be used for any speaker adaptation method.\"\n\nQuestion: what is the main goal", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head attention which is proposed to capture the localness and directional information of self-attention based encoder. It is a function to map queries and key-value pairs to the representation of input. It is similar as scaled dot-product attention but it uses Gaussian weight matrix to adjust the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters.\n\nQuestion: What is the difference between Gaussian-masked directional multi-head attention and standard multi-head attention?\n\nAnswer: Gaussian-", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "unanswerable (Document[10]):\"We first discuss the transfer results from BERT . Initialized from fastText vectors, RAMEN$_{\\textsc {base}}$ slightly outperforms mBERT by 1.9 points on average and widen the gap of 3.3 points on Arabic . RAMEN$_{\\textsc {base}}$ gains extra 0.8 points on average when initialized from parallel data . With triple number of parameters, RAMEN$_{\\textsc {large}}$ offers an additional boost in term of accuracy and initialization with parallel data consistently improves the performance . It has been shown", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "unanswerable (Document[10]):\"In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general . The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?\"\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer: unanswerable (Document[10]):\"In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "RNN (Recurrent Neural Network) (Document[11]):\"In our experiments, the RNN takes in spectrograms of utterances, passing them through two 2D - convolutional layers, followed by seven bi-directional recurrent layers and a fully - connected layer with softmax activation . All recurrent layers are batch normalized . At each timestep, the softmax activations give a probability distribution over the characters . CTC loss BIBREF8 is then computed from the timestep - wise probabilities.\"\n\nAnswer: RNN (Document[11]):", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "BiLSTM (Bidirectional LSTM) (Document[20]):\"In this section, we describe our approach in building our model . This model is partly inspired from multiple models BIBREF20,BIBREF1, andBIBREF2\nApproach ::: Bidirectional LSTM\nWe used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence . Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state . However, it is proven benef", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "2020 (Document[10]):\"This paper is accepted to the 2020 IEEE International Conference on Data Mining (ICDM 2020) and will be published in the proceedings of the conference . The conference will be held on November 15--18, 2020 in Brisbane, Australia.\"\n\nAnswer: 2020\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: \"We propose a natural language generation model based on pre-trained language models (we use BERT in", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "500 (Document[37]):\"We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts . Workers on a crowd - sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer . Figure FIGREF34 shows the instructions they were given . Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "SVMs (Support Vector Machines) and neural networks (BIBREF18)\n\nQuestion: What is the performance of the SVM model?\n\nAnswer: chance (BIBREF18)\n\nQuestion: What is the performance of the CNN model?\n\nAnswer: a macro-F1 score of 0.80 (BIBREF19)\n\nQuestion: What is the performance of the BiLSTM model?\n\nAnswer: a macro-F1 score of 0.69 (BIBREF19)\n\nQuestion: What is the performance", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLSTM+CNN(grapheme-level) (Document[10]):\"In this paper, we present works similar to ours in Section SECREF2 . We describe our approach and dataset statistics in Section SECREF3 and SECREF4, followed by our experiments, evaluation and discussion in Section SECREF5, SECREF6, and SECREF7 . We conclude with our observations in Section SECREF8 .\nTo facilitate further research our code and dataset will be made available at github.com/link-yet-to-be-updated\nRelated", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "UTD (Unsupervised Term Discovery) (Section SECREF1)\n\nQuestion: What is the difference between UTD and AUD?\n\nAnswer: UTD is a unsupervised approach that operates directly on the speech of interest, while AUD is a supervised approach that uses a pre-trained acoustic model to discover acoustic units (phonemes) from untranscribed speech (Section SECREF1)\n\nQuestion: What is the difference between UTD and ASR?\n\nAnswer: UTD does not require transcribed speech, while ASR requires trans", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERTbase (Document[30]):\"The proposed method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768 . To accelerate the training speed, two - phase training BIBREF1 is adopted . The first phase uses a maximal sentence length of 128, and 512 for the second phase . The numbers of training steps of two phases are 50K and 40K for the BERTBase model . We used AdamW BIBREF13 optimizer with a", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "five keyphrase extraction models (TopicRank, WINGNUS, KP-miner, TF-IDF, and KP-miner) (Document[15]):\"In this paper, we re-assess the performance of several state - of - the - art keyphrase extraction models at increasingly sophisticated levels of preprocessing . Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document . In doing so, we present the first", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "yes\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: Related Works\nSeveral studies have addressed the NMT-specific problem of missing or repeating words. Niehues2016 optimized NMT by adding the outputs of PBSMT to the input of NMT. Mi2016a and Feng2016 introduced a distributed version of coverage vector taken from PBSMT to consider which words have been already translated. All these methods, including ours, employ information of the source sentence to improve the quality of translation, but our method uses back-translation", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIMIC-III is a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The database contains all of the notes associated with each patient's time spent in the ICU as well as 55,177 discharge reports and 4,475 discharge addendums for 41,127 distinct patients. Only the original discharge reports were included in our analyses. Each discharge summary was divided into", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "14,100 (Document[16]):\"OLID is a new dataset with annotation of type and target of offensive language . OLID is the official dataset of the shared task SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval) BIBREF16 . In OffensEval, each annotation level in OLID is an independent sub-task . The dataset contains 14,100 tweets and is released freely to the research community.\"\n\nAnswer: 14,", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "the correct translation of verbs requires the systems to pay attention to different parts of the source sentence (Section SECREF27).\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer: attention does not necessarily have to follow alignments (Section SECREF27).\n\nQuestion: What is the difference between attention and alignment in different syntactic phenomena?\n\nAnswer: attention agrees with traditional alignments to a high degree in the case of nouns (Section SECREF27).\n\nQuestion: What is the difference between attention and alignment in different POS tags", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "BOW and TFIDF (Document[19]):\"The baseline embedding methods are including bag - of - words (BOW), term frequency--inverse document frequency (TFIDF), and neural - based word embedding . The classification models are including Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe BIBREF11, and our proposed model.\"\n\nQuestion: what is the proposed model?\n\nAnswer: FriendsBERT (Document[19]):\"The proposed FriendsBERT successfully", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "19 different algorithms (Document[10]):\"In this section, we describe our dataset as well as our tag recommendation approaches we propose to annotate e-books .\nDataset . Our dataset contains two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations . HGV GmbH has collected all data sources and we provide the dataset statistics in Table TABREF3 .\nData used to generate recommendations . We employ two sources of e-book annotation data: (i) editor tags, and (ii) Amazon search terms . For editor tags, we collect data of 4", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "end-to-end one-to-many and many-to-many multilingual ST models (Document[8]):\"End-to-end one-to-many and many-to-many multilingual ST models have been explored very recently . However, many-to-one multilingual models, to our knowledge, have not . We hence use CoVoST to examine this setting.\"\n\nQuestion: What is the purpose of the corpus?\n\nAnswer: to bridge the performance gaps between end-to-end models and cascading systems (Document[1]):", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 (Document[23]):\"To showcase the effectiveness of the PRU, we evaluate the performance on two standard datasets for word - level language modeling and compare with state - of - the-art methods . Additionally, we provide a detailed examination of the PRU and its behavior on the language modeling tasks.\"\n\nQuestion: what is the main idea of the article?\n\nAnswer: Pyramidal Recurrent Units", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "unanswerable\n\nQuestion: Do they use clustering?\n\nAnswer: yes\n\nQuestion: Do they use machine learning?\n\nAnswer: yes\n\nQuestion: Do they use deep learning?\n\nAnswer: unanswerable\n\nQuestion: Do they use natural language processing?\n\nAnswer: yes\n\nQuestion: Do they use text classification?\n\nAnswer: yes\n\nQuestion: Do they use text clustering?\n\nAnswer: yes\n\nQuestion: Do they use text summarization?\n\nAnswer: unanswerable\n\nQuestion: Do they use text generation?\n\nAnswer", "answers": ["No", "No"], "all_classes": null, "length": 3158}

{"pred": "FoxNews (Foxford)\n\n\n\nQuestion: What is the name of the dataset they used?\n\nAnswer:Affective\n\n\nQuestion: What is the name of the model they used?\n\nAnswer:B-M\n\nQuestion: What is the name of the feature they used?\n\nAnswer:B-M\n\nQuestion: What is the name of the model they used?\n\nAnswer:B-M\n\nQuestion: What is the name of the model they used?\n\nAnswer:B-M\n\nQuestion: What is the name of the model they used", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "linear contiguous series of posts (Posts) (Document[111]):\"The latent context is a series of posts (Posts) that are intervened by an instructor . The latent context is a series of posts (Posts) that are intervened by an instructor . The latent context is a series of posts (Posts) that are intervened by an instructor . The latent context is a series of posts (Posts) that are intervened by an instructor . The latent context is a series of posts (Posts) that are intervened by an instruct", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-\n\n\n\nF1. Classification Accuracy\n2. Precision\n3. F1 Score\n4. Kappa\n5. ROC\n6. AUC\n7. MCC\n8. MSE\n9. RMSE\n10. R2\n111. RMSE\n12. RMSE\n13. RMSE\n14. RMSE\n15. RMSE\n16. RMSE\n17. RMSE\n18. RMSE\n19. RMSE\n", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "plain stacked\n\n\n\n\n\nQuestion: What was the proposed method?\n\n\nAnswer:\n\n\nQuestion: What was the experimental setup?\n\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was the experimental setup?\n\nAnswer:\n\nQuestion: What was", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "unanswerable (Document[17]):\"jiant is released under the MIT License BIBREF22 . It is compatible with PyTorch BIBREF33 and TensorFlow BIBREF4 . It is also compatible with Hugging Face Transformers BIBREF5 . It is compatible with AllenNLP BIBREF6 . It is compatible with the Transformers library BIBREF7 . It is compatible with the AllenNLP library BIBREF8 . It is compatible with the Transformers library BIBREF9 . It is compatible with the AllenNLP library B", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "yes (unanswerable)\n\n\n\nQuestion: What is the purpose of the PrivacyQA dataset?\n\nAnswer:to kickstart research in the field of privacy question answering (§SECREF3)\n\nQuestion: What is the size of the PrivacyQA dataset?\n\nAnswer:1750000000000000000000000000000000000000000000000000000000000", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "unanswerable\n\n\nThe results in Table TABREF12 show that shallow syntactic structures do not improve sentiment classification.\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\nunanswerable", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon reviews from Yelastic (Amazon) (Bib) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "7.3% (Document[111]):\"We then conducted a comprehensive comparison of the baseline model and our proposed model, and the results are shown in Table TABREF1 . We can see that the improvements of small - scale unbalanced datasets when sentence representation is enhanced with topic information are 7.3% on accuracy and 9.6% on F1 score.\"\n\nAnswer:7.3% (Document[1]):\"We then conducted a comprehensive comparison of the baseline model and our proposed model, and the results are shown in Table TABREF1", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "transcribed text (Document[1]):\"We evaluate our model using the IEMOC 2015 dataset, which contains 100 hours of audio and 100 hours of transcripts . We use the same evaluation protocol as in the previous work . We report the macro F1 score, which is the average of the F1 scores for each class . We also report the accuracy, which is the number of correctly classified samples divided by the total number of samples . We compare our results with the previous work using the same dataset . We also compare our results with the previous work using the same dataset", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUTOur\n\n\n\nQuestion:What is the purpose of the sentence classification?\n\nAnswer:to identify strengths, weaknesses and suggestions\n\nQuestion:What is the purpose of the multi-class classification?\n\nAnswer:to match supervisor assessments to pre-defined perspectives\n\nQuestion:What is the purpose of the summarization?\n\nAnswer:to create a summary of the peer feedback\n\nQuestion:What is the purpose of the multi-class classification?\n\nAnswer:to match supervisor assessments to pre-defined perspectives\n\nQuestion:What is", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT (BERT5)\n\n\n\nQuestion: What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "pivoting\n\n\n\nPivot-based Method is a common strategy to obtain a source-target model for a low-resource language pair, which first translates the source language to a pivot language and then translates the pivot language to the target language. This method is further divided into pivoting and pivot-synthetic. While pivoting translates the source language to the pivot language directly, pivot-synthetic first synthesizes the pivot language from the source language and then translates the synthetic language to the target language.\n\nPivot-based Method is a common strategy to obtain a source-target", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "EmotionLines 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "several evaluation protocols for each part of the database. (Document[211]):\"The database is provided with several evaluation protocols for each part of the database . The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of duration, channel, and speaker on the performance . The protocols also allow researchers to study the effects of channel and speaker on the performance . The protocols also allow researchers to study the effects of channel and speaker on the performance . The protocols also allow researchers to study the effects of channel and speaker on the performance . The protocols also", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian- The Gaussian-masked directional multi-head attention is a variant of multi-head attention that uses a Gaussian mask to control the attention weights between different heads. The Gaussian mask is applied to the query, key, and value matrices, and is learned during training. The Gaussian mask is used to control the attention weights between different heads, making the model more robust to noise and able to better handle long-range dependencies.\n\nAnswer:unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is the introduction of Gaussian-masked directional multi", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "unanswerable\n\n\nThe current system uses parallel data to initialize the foreign embeddings.\n\n\nAnswer:\nunanswerable\n\nQuestion: How much training data from the non-English language is used by the system?\n\nAnswer:unanswerable\n\nQuestion: How much training data from the non-English language is used by the system?\n\nAnswer:unanswerable\n\nQuestion: How much training data from the non-English language is used by the system?\n\nAnswer:unanswerable\n\nQuestion: How much training data from the non-English language is used by the", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "unanswered\n\n\nIn this paper, we show that attention and alignment are not the same. We show that attention is not only capable of modelling alignment but also captures information beyond alignment. We show that attention is not always aligned with alignments. We show that attention is more concentrated on the aligned words for nouns and more distributed for the other parts of speech.\n\nAnswer:\nIn this paper, we show that attention and alignment are not the same. We show that attention is not only capable of modelling alignment but also captures information beyond alignment. We show that attention is not always aligned with", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "RNN (Document[111]):\"In this paper, we propose a novel active learning method for end-to-end speech recognition, which is based on the Expected Gradient Length (EGL) BIBREF3 . EGL is a gradient - based measure of the model's uncertainty, which is used to select the most informative samples for labeling . We show that EGL is superior to confidence - based methods, and that it can be used to effectively reduce the number of samples required for training . We also show that EGL is not correlated with the confidence scores used by the model", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "BiLSTAN\n\n\n\nQuestion:What is the dataset?\n\nAnswer:OurNepali\n\nQuestion:What is the model?\nAnswer:BiLSTM\n\nQuestion:What is the architecture?\nAnswer:BiLSTM\n\nQuestion:What is the evaluation metric?\nAnswer:F1\n\nQuestion:What is the improvement?\nAnswer:10%\n\nQuestion:What is the improvement?\nAnswer:10%\n\nQuestion:What is the improvement?\nAnswer:10%\n\nQuestion:What is the improvement?\nAnswer:", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "20101/01/2020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "50 (Document[5]):\"We collected human judgments on the quality of the 16 generated texts from 5 people . The judgments were collected using a web-based interface, and were based on the following criteria: 1) Accuracy: how well the text describes the table 2) Fluency: how well the text flows and is easy to read 3) Completeness: how much of the table is covered in the text 4) Overall quality: how good the text is overall\"\n\nQuestion: What is the name of the dataset used in the", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "SVM (SVM)\n\n\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best model?\n\nAnswer:SVM\n\nQuestion: What is the best", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLSTMost of the models experimented models are based on the architecture of Bidirectional LSTM (BiLSTM) with different types of embeddings. The embeddings used are:\n\n1. Word2Vec\n2. GloVe\n3. FastText\n4. Word2Vec with subword embeddings\n5. Word2Vec with subword embeddings and POS tags\n6. GloVe with subword embeddings and POS tags\n7. FastText with subword embeddings and POS tags\n8. Word2Vec", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "UTD (Section SECREF)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERTBase\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "five keyphrase\n\n\nQuestion: what is the SemEval-2010 benchmark dataset?\n\nAnswer:BIBF00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "yes\n\n\n\nIn this paper, we evaluate the effectiveness of pre-training by comparing it with a jointly-trained model of forward translation and back-translation. Experimental results show that the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training.\n\nAnswer:\nunanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nRelated Works\nSeveral studies have addressed", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIMDBLP-MIMDB-10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "14,141000\n\n\nOLID is a new dataset for offensive language identification in social media. It contains 14,100 tweets annotated with three levels of annotation: offensive language (level A), target of offensive language (level B), and type of offensive language (level C). The dataset is split into training (10,000 tweets) and testing (4,100 tweets). The dataset is released freely to the research community.\n\nOLID is a new dataset for offensive language identification in social media. It", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "the relevance of the attended parts, especially cases where attention is \"smeared out\" over multiple words (BIBREF0)\n\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:Attention models traditional alignments in some cases while capturing other information beyond alignments in others (BIBREF0)\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:Attention models traditional alignments in some cases while capturing other information beyond alignments in others (BIBREF0)\n\nQuestion: What is the difference between attention and alignment?\n\n", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "BOW\n\n\nThe baseline embedding methods are including bag-of-words (BOW) and term frequency-inverse document frequency (TF-IDF)\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "19 (Document[9]):\"In this paper, we propose a novel approach to annotate e-books with tags, which are suitable for readers and search engines . We collect editorial tags from 48,0000 e-books from 13 different publishers and use them to train our tag recommendation system . We furthermore collect Amazon search terms from 1,000,0000 e-books and use them to train our tag recommendation system . We evaluate our tag recommendation system on a dataset of 1,000,000 e-books and compare", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "end- ASRNN (encoder) - ASRNN (decoder)\n\n\nAnswer:end-to-end\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n\nAnswer:yes\n", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Pennu (Document[1]\n\n\"Following recent work, we train our model on the Penn Treebank dataset BIBREF28 . We use the same preprocessing as in BIBREF28 . We use the same validation and test splits as in BIBREF28 . We use the same hyperparameters as in BIBREF28 . We use the same model architecture as in BIBREF28 . We use the same optimizer as in BIBREF28 . We use the same learning rate schedule as in BIBREF28 . We use the same drop", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "unanswerable\n\n\n\nQuestion: Do they use clustering?\n\nAnswer:yes\n\nQuestion: Do they use SVM?\n\nAnswer:yes\n\nQuestion: Do they use SVM for clustering?\n\nAnswer:yes\n\nQuestion: Do they use SVM for classification?\n\nAnswer:yes\n\nQuestion: Do they use SVM for regression?\n\nAnswer:no\n\nQuestion: Do they use SVM for clustering and classification?\n\nAnswer:yes\n\nQuestion: Do they use SVM for regression and clustering?\n\nAnswer:no\n", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "\"Since the student and teacher models' output spaces are incompatible, the student model's output space is not a subset of the teacher model's output space, and the student model's vocabulary is not a subset of the teacher model's vocabulary, the student model's softmax layer is not able to predict the teacher model's output\" (SEC: 2).\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "word20.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "1.70000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "unanswerable\n\n\n\nIntroduction\nFrom the article:\n\"Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answerability is a measure of the likelihood that a question will be answered. Answer", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "words embeddifferent kinds of features (Document[10]):\"Features . We employ a rich set of features borrowed from previous works on fake news detection, rumors detection, and propaganda detection . We use the following features: Emotion: We use two lexicons of emotions: one is based on Plutchik's wheel of emotions BIBREF22 and the other is based on Ekman's six basic emotions BIBREF23 . Morality: We use a set of words that are related to moral values BIBREF24 . Style: We use a set of", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "XN/\n\n\n\nWe evaluate our approach for six target languages: French (fr), German (de), Italian (it), Spanish (es), Russian (ru), Chinese (zh), and Japanese (ja).\n\nWe use the following datasets for evaluation:\n\n\n1.\n\n\n2.\n\n3.\n\n4.\n\n5.\n\n6.\n\n7.\n\n8.\n\n9.\n\n10.\n\n11.\n\n12.\n\n13.\n\n14.\n\n15.\n\n16.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable\n\nEnglish (BIBM)\n\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish (IBM)\n\nAnswer:\nEnglish", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "unanswerable\n\n\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "over 450000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "ELMoSeveral (Document[111]):\"We conducted a comprehensive comparison with the baseline models, and some of them were never investigated for the abbreviation disambiguation task . We applied traditional features by simply taking TF-IDF features as the inputs to four classifiers: support vector machine (SVM), Naive Bayes, logistic regression and random forest . We also applied deep features by fine - tuning the pre-trained Doc2Vec model BIBREF20 to the abbreviation disambiguation task . We also applied the Doc2Vec model to the abbreviation disambiguation", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "2519 (Document[111]):\"The EEG signals were recorded from 5 scalp electrodes (Fz, Cz, Pz, T7 and T8) using a 10--20 system, with impedance of 50 kΩ and reference electrode placed at Cz . The signals were recorded at a sampling rate of 500 Hz and filtered with a band pass filter of 0.01--40 Hz . The signals were then re-referenced to the average reference signal and filtered with a band pass filter of 0.0", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Retrievalence (Document[17]):\"Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational dialogue . Each interaction in Macaw (from both user and system) is a Message object, thus a conversation is a list of Messages . Macaw consists of multiple actions, each action is a module that can satisfy the information needs of users . For example, search and question answering are two actions in Macaw . Even multiple search algorithms can be also seen as multiple actions . Each action can produce multiple outputs", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "yes (unanswerable)\n\n\nAnswer: yes\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable (unanswerable)\n\n\n\nQuestion: what languages are present in the corpus?\n\nAnswer:answerable\n\nQuestion: what dialects are present in the corpus?\n\nAnswer:answerable\n\nQuestion: what dialects are present in the corpus?\n\nAnswer:unanswerable\n\nQuestion: what dialects are present in the corpus?\n\nAnswer:unanswerable\n\nQuestion: what dialects are present in the corpus?\n\nAnswer:unanswerable\n\nQuestion: what dialects are present in the corpus?\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "SQuestions are performed on the SQuAD dataset BIBREF3. It contains 1000,00000 question-answer pairs, where each question is a span of text in the corresponding article. (Document[10]):\"We conduct experiments on the SQuAD dataset BREF3 . It contains 100,0000 question - answer pairs, where each question is a span of text in the corresponding article.\"\n\nQuestion:What is the task of the proposed model?\n\nAnswer:Question Generation (QG) (Document[10]):\"", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "SemEarX\n\n\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model?\n\nAnswer:ER\n\nQuestion:What is the best performing model", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "\n\nThe training corpus consisted of the English Wikipedia dump from February 2017 (English Wikipedia) BIBREF0 and the Russian Wikipedia dump from December 2017 (Russian Wikipedia) BIBREF1.\n\nAnswer:\nunanswerable\n\nQuestion: What was the size of the corpora they trained ELMo on?\n\nAnswer:\nThe training corpus consisted of the English Wikipedia dump from February 2017 (English Wikipedia) BIBREF0 and the Russian Wikipedia dump from December 2017 (Russian Wikipedia) BIBREF1.", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "GMini\n\n\nWe train our proposed model GMKL on the Text8 dataset BIBREF14 which is a pre-processed data of 17M words from wikipedia. Of which 7M words are frequent and 10M words are infrequent. We compare GMKL with previous approaches w2v BIBREF11 and GloVe BIBREF12. For all the models used for experimentation, the embedding size ($D$) was set to 50. We used the same hyper-parameters as in BIBREF11 and B", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "significant performance boost (Document[10]):\"Experformance boosts on paraphrase identification tasks\"\n\n\nQuestion: What are method improvements of F1 for named entity recognition?\n\nAnswer: significant (Document[1]):\"Experiments on NER tasks show that the proposed method can achieve significant performance boosts on paraphrase identification tasks.\"\n\nQuestion: What are method improvements of F1 for machine reading comprehension?\n\nAnswer: significant (Document[1]):\"Experiments on MRC tasks show that the proposed method can achieve significant performance boosts on par", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "\n\n\nThe gradient-based methods, including EGL, are based on the idea of variance reduction. Specifically, they aim to reduce the variance of the estimator of the model parameters. The estimator is typically the log-likelihood of the training data, which is a sum of the log-likelihoods of the training instances. The gradient of the log-likelihood with respect to the model parameters is the negative of the gradient of the log-likelihood with respect to the training data. The gradient-based methods aim to reduce the variance of the estimator by minimizing the norm of the gradient.\n\n", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "LSTMost of the CoNLL--SIGMORPHON 2018 shared task on universal morphological BIBREF0 . This paper describes our approach and results for Task 2 of the CoNLL--SIGMORPHON 2018 shared task on universal morphological BIBREF0 . The task is to generate a morphological analysis of a word given its lemma and context.\n\nArticle:Introduction\nThis paper describes our approach and results for Task 2 of the CoNLL--SIGMORPHON 2018 shared task on universal morphological analysis", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "natural language understanding and reasoning questions (Document[10]):\"The dataset is composed of 5000,0000 reviews from TripAdvisor and 100,0000 questions generated by crowdsourcing . The questions are divided into 8 different categories: 1) understanding and reasoning questions (Document[10]):\"The questions are divided into 8 different categories: 1) understanding and reasoning questions (Document[10]):\"The questions are divided into 8 different categories: 1) understanding and reasoning questions (Document[10]):\"", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes (BERT)\n\n\n\nQuestion: What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "533 (Document)\n\n\nQuestion: How many sentences are in the new corpus?\n\nAnswer:53 (Sentence)\n\nQuestion: How many tokens are in the new corpus?\n\nAnswer:167 (Token)\n\nQuestion: How many words are in the new corpus?\n\nAnswer:19 (Word)\n\nQuestion: How many entities are in the new corpus?\n\nAnswer:5 (Entity)\n\nQuestion: How many relations are in the new corpus?\n\nAnswer:5 (Relation)\n\nQuestion", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food. We collect a novel dataset of 18000K+ recipes and 1M+ user reviews from Food.com (BIBREF0).\n\nQuestion: How many users are there?\n\nAnswer:1800K+ (BIBREF0).\n\nQuestion: How many recipes are there?\n\nAnswer:1M+ (BIBREF0).\n\nQuestion: What is the purpose of the dataset?\n\nAnswer:To train a model that can generate recipes based on user preferences (BIBREF0).\n\nQuestion: What is the", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "tab:intrough we only statistically significant ( $p < 0.05$ under a random permutation of the labels)\n\nAnswer:intrinsic evaluation via word similarity and word analogy tasks\n\nAnswer:downstream tasks from the VecEval suite\n\nAnswer:quantitative evaluation of nearest neighbors\n\nAnswer:statistically significant (p < 0.05)\n\nAnswer:statistically significant (p < 0.05)\n\nAnswer:statistically significant (p < 0.05)\n\nAnswer:statistically significant", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "unanswerable\n\n\n10   None\n1   None\n2   Unsure\n3   None\n4   Unsure\n5   None\n6   Unsure\n7   None\n8   Unsure\n9   None\n10   Unsure\n11   None\n12   Unsure\n13   None\n14   Unsure\n15   None\n16   Unsure\n17   None\n18   Unsure\n19   None\n20   Unsure\n21   None\n2", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "150\n\n\n\nArticle:Introduction\nSeveral learner corpora have been compiled for English, such as the International Corpus of Learner English BIBREF0, the British Academic Spoken English Corpus BIBREF1, and the International Corpus of English Language Teaching BIBREF2. However, there are no learner corpora for Portuguese.\nIn this paper, we present the first learner corpus for Portuguese, the Corpus of Portuguese as a Foreign Language (CPFL). The corpus consists of 100 texts written by learners of Portuguese as", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "textual patterns learned from an annotated corpus (Section SECREF1)\n\n\nQuestion: What is the purpose of the error generation system?\n\nAnswer:to create artificial data for training error detection models (Section SECREF2)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:two supervised error generation methods (Section SECREF3)\n\nQuestion: What is the main focus of the paper?\n\nAnswer:two supervised error generation methods (Section SECREF3)\n\nQuestion: What is the main goal of the paper?\n\nAnswer", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "unanswerable\n\n\nThe concatenation of the word embeddings of the two words is a vector of length 2d, which is then projected to a vector of length 2d-1 by a linear transformation. The resulting vector is then normalized to have unit length.\n\nThe concatenation of the word embeddings of the two words is a vector of length 2d, which is then projected to a vector of length 2d-1 by a linear transformation. The resulting vector is then normalized to have unit length.\n\nThe concatenation of the word embeddings of", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "RKS approach explicitly maps embedding vectors to a higher dimensional space using Random Kitchen Sink (RKS) approach.\n\n\nAnswer:\nRandom Kitchen Sink (RKS) approach is a method to map the embedding vectors to a higher dimensional space.\n\nAnswer:\nRandom Kitchen Sink (RKS) approach is a method to map the embedding vectors to a higher dimensional space.\n\nAnswer:\nRandom Kitchen Sink (RKS) approach is a method to map the embedding vectors to a higher dimensional space.\n\nAnswer:\nRandom Kitchen Sink (RKS) approach is", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "a variant where we share token embeddings between encoder and decoder (BIBREF27)\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo embeddings input to the encoder (BIBREF10)\n\nQuestion: What is the best performing strategy for machine translation?\n\nAnswer: ELMo embeddings input to the encoder (BIBREF10)\n\nQuestion: What is the best performing strategy for abstractive summarization?\n\nAnswer: ELMo embeddings input to the encoder (BIBREF10)\n\n", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The results show that error generation systems can improve error detection performance by a large margin, and that the best results are achieved by combining the two approaches.\n\nAnswer: The results show that each method provides significant improvements over using only the available training data, and a combination of both gives an absolute improvement of 4.3% on the FCE dataset and 4.3% on the CoNLL dataset.\n\nAnswer: The results show that error generation systems can improve error detection performance by a large margin, and that the best results are achieved by combining the two approaches.\n\nAnswer: The results show that error generation", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "none (unanswerable)\n\n\nQuestion: What is the name of the model?\n\nAnswer:tweet2vec (unanswerable)\n\nQuestion: What is the name of the dataset?\n\nAnswer:Twitter (unanswerable)\n\nQuestion: What is the name of the model?\n\nAnswer:tweet2vec (unanswerable)\n\nQuestion: What is the name of the model?\n\nAnswer:tweet2vec (unanswerable)\n\nQuestion: What is the name of the model?\n\nAnswer:tweet2vec (unanswer", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "seven experts with legal training (Document[1]):\"We recruit seven experts with legal training to annotate the answers to the questions asked by the crowdworkers . The experts were asked to identify relevant evidence within the privacy policy, and to provide meta-annotation on the question, including its relevance to the policy, whether the question is subjective, and whether the question is answerable . The experts were also asked to identify the category of the question, based on the OPS scheme . The experts were asked to identify the category of the question, based on the OPS scheme . The exper", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "machine learning-based (Based (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (Based) (", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford'Stan evaluation of 3 named entity recognition models: Stanford NER, spacy, and CharNer' (Document[19]):\"We evaluated 3 named entity recognition models: Stanford NER, spacy, and CharNer.\"\n\nQuestion: what is the name of the dataset used for the evaluation?\n\nAnswer:'the manually annotated dataset' (Document[19]):\"We used the manually annotated dataset for evaluation.\"\n\nQuestion: what is the name of the corpus used for the evaluation?\n\nAnswer:'the manually annotated", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN/\n\n\n\nWe use two popular knowledge bases: WordNet B-3M and Freebase.\n\nWe use two subsets of WordNet B-3M: WN18 and WN18R.\nWe use two subsets of Freebase: FB13 and FB13R.\nWe use two subsets of FB13: FB13 and FB13R.\nWe use two subsets of FB13: FB13 and FB13R.\nWe use two subsets of FB13: FB13 and FB13R.\nWe", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "LastState-RNN (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastStateRNN) (LastState", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "three topics of cyberbullying (Document[1]):\"Cyberbullying has been defined by the National Crime Prevention Council as the use of the Internet or other electronic means to bully a person, in a place where that person is at a disadvantage to defend themselves . Cyberbullying can be either a deliberate attempt to hurt someone or a careless act . Cyberbullying can be done by anyone, including children, teenagers, adults, and the elderly . Cyberbullying can be done by sending messages, posting information, or spreading rumors about a person using", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "yes\n\n\n\"Results ::: Content Analysis\nAfter pre-processing, we have 20 articles from ISIS and 13 articles from Catholic women's forum. We have 10 articles from ISIS that address women and 3 articles from Catholic women's forum that address women. We have 10 articles from ISIS that do not address women and 3 articles from Catholic women's forum that do not address women.\n\nWe have 10 articles from ISIS that address women and 3 articles from Catholic women's forum that address women. We have 10 articles from", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "an annotated Twitter dataset that was constructed based on a hierarchical model of depression BIBREF12, BIBREF13 . The dataset contains 9,473 annotations for 9,393 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizenship is a great way to get involved in your community\") or evidence of depression (e.g., “I'm feeling depressed today”). If a tweet is annotated as evidence of depression, then it is further annotated as one of the following", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "Nguni (zul, xho, nbl, ssw, ssw, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn, tsn", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IMDb dataset of movie reviews (BIBMDb) (Document[111]):\"The IMDb dataset used has a total of 25,0000 sentences, with half of them being positive and the other half negative . The dataset is balanced.\"\n\nQuestion: What is the size of the GMB corpus?\n\nAnswer: 3.5 million words (Document[10]):\"The GMB corpus is a 3.5 million word corpus of English text, which is used to train the word embeddings . The corpus is a collection of text from the", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "8999.6%\n\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n\nAnswer:8.6%\n\n", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K- K- K-means\n\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Related Work\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Related Work\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Related Work\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Related Work\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Related Work\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade) (Bibref12).\n\n\nAnswer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade) (Bibref12).\n\nAnswer: English (Edinburgh), German (Berlin), Spanish", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "Table TABSTREFREF10\n\n\nQuestion: What is the purpose of the system?\n\nAnswer:STREF1\n\nQuestion: What is the problem formulation?\n\nAnswer:STREF2\n\nQuestion: What is the similarity metric used?\n\nAnswer:STREF3\n\nQuestion: What is the document representation?\n\nAnswer:STREF4\n\nQuestion: What is the similarity metric used?\n\nAnswer:STREF3\n\nQuestion: What is the similarity metric used?\n\nAnswer:STREF3\n\nQuestion:", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "unanswerable\n\n\nThe lexicon is built over all the data and therefore includes the vocabulary from both the training and testing data.\n\n\nQuestion: Is the lexicon a dictionary?\n\nAnswer:\nunanswerable\n\nThe lexicon is not a dictionary. It is a set of words that are used to represent the language.\n\nQuestion: Is the lexicon a list of words?\n\nAnswer:\nunanswerable\n\nThe lexicon is not a list of words. It is a set of words that are used to represent the language.\n\nQuestion: Is the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "average GloVan average GloVe\n\n\n\nQuestion:What is the performance of SBERT on the STS benchmark?\n\nAnswer:outperforms\n\nQuestion:What is the performance of SBERT on the STS benchmark?\n\nAnswer:outperforms\n\nQuestion:What is the performance of SBERT on the STS benchmark?\n\nAnswer:outperforms\n\nQuestion:What is the performance of SBERT on the STS benchmark?\n\nAnswer:outperforms\n\nQuestion:What is the performance of SBERT on the STS benchmark?", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "SentEvaluated different pooling different transfer learning tasks (Table TABREF1)\n\n\nQuestion:What is the purpose of the paper?\n\nAnswer:To present Sentence-BERT, a method to derive semantically meaningful sentence embeddings from BERT (Table TABREF1)\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:To present Sentence-BERT, a method to derive semantically meaningful sentence embeddings from BERT (Table TABREF1)\n\nQuestion:What is the main focus of the paper?\n", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "30.5 million (Document[19]):\"The vocabulary of users may differ from the one of editors . For example, while editors annotate e-books with tags that are semantically related to the content of the book, users search for books with tags that are related to the title of the book . In this paper, we propose to combine the vocabulary of users and editors to improve the quality of tag recommendations.\"\n\nAnswer:unanswerable\n\nQuestion: how many e-books are annotated?\n\nAnswer:48,75", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context tweets\n\n\n\nQuestion: What is the conclusion of the article?\n\nAnswer:The best model for detecting abusive language is a bidirectional LSTM with attention mechanism.\n\nQuestion: What is the best model for detecting abusive language?\n\nAnswer:bidirectional LSTM with attention mechanism\n\nQuestion: What is the best model for detecting abusive language?\n\nAnswer:bidirectional LSTM with attention mechanism\n\nQuestion: What is the best model for detecting abusive language?\n\nAnswer:bidirectional L", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "yes\n\n\n\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "yes\n\n\n\"In this paper, we present a method to automatically detect demographic, linguistic, and psycholinguistic dimensions of people from their blogs. We use a large dataset of blogs written by people in the U.S. and analyze the linguistic content of the blogs to extract information about the authors. We use a combination of machine learning and natural language processing techniques to identify the demographic, linguistic, and psycholinguistic dimensions of the authors. The results of our analysis show that we can accurately identify the demographic, linguistic, and psycholinguistic dimensions of people from", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "3rd (unanswerable)\n\n\nQuestion: What is the best performing model among all the submissions?\n\nAnswer:3rd (unanswerable)\n\nQuestion: What is the best performing model among all the submissions?\n\nAnswer:3rd (unanswerable)\n\nQuestion: What is the best performing model among all the submissions?\n\nAnswer:3rd (unanswerable)\n\nQuestion: What is the best performing model among all the submissions?\n\nAnswer:3rd (unanswerable)\n\nQuestion: What is the best performing model among", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "DTA122\n\n\nQuestion:What is the evaluation metric used for the task?\n\nAnswer:TA2\n\nQuestion:What is the output of the system?\n\nAnswer:TA2\n\nQuestion:What is the goal of the shared task?\n\nAnswer:TA2\n\nQuestion:What is the benchmark for the shared task?\n\nAnswer:TA2\n\nQuestion:What is the number of teams participating in the shared task?\n\nAnswer:12\n\nQuestion:What is the number of participants in the shared task?\n\nAnswer:12\n\n", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "10\n\n\n\n\nAnswer:10\n\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer:\n10\n\nAnswer", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K Nearestimplementing a car-speak\n\n\nQuestion: What is the main goal of the paper?\n\nAnswer:implementing car-speak\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:implementing car-speak\n\nQuestion: What is the main focus of the paper?\n\nAnswer:implementing car-speak\n\nQuestion: What is the main topic of the paper?\n\nAnswer:implementing car-speak\n\nQuestion: What is the main idea of the paper?\n\nAnswer:implementing", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "Extended middle context is split into three parts: left, middle, and right contexts (BIBM, 2014). The left and right contexts are defined as the parts of the sentence before and after the entity mention, respectively. The middle context is defined as the part of the sentence between the entity mention and the relation argument (BIBM, 2014).\n\nAnswer:The middle context is defined as the part of the sentence between the entity mention and the relation argument (BIBM, 2014).\n\nAnswer:The middle context is defined as the part", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "multi-\n\n\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-granularity:\nmulti-gr", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "over 450000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "2600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "BIBMost recent work has focused on learning tree-structured representations of sentences, which are then used to induce a grammar. The most common approach is to learn a tree-structured representation of a sentence via a neural network, and then use the tree to induce a grammar. BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIB", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The neutral word recognition as a defense against adversarial attacks (Document[111]):\"The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are used to neutralize the effects of adversarial attacks . The backoff strategies are", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "DSTTeach dialogue-level data is annotated with the user's goal, the system's response, and the user's response to the system's response. (Document[11]):\"The corpus used for training the NUS is the DSTC2 corpus, which consists of 16000 dialogues between 10000 users and 10000 system responses . The dialogues are annotated with the user's goal, the system's response, and the user's response to the system's response . The goal is", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEADVoccurve (BLEU)\n\n\nAnswer:BLEU\n\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\nAnswer:BLEU\n\n", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "AutoJudge (Document[10]):\"Inspite of the datasets, we compare our model with the following state - of - the - art models:\n\nSVM: A simple SVM model with lexical features and bag - of - words features.\nGRU: A GRU model with the same features as SVM.\nGRU+Lex: A GRU model with the same features as SVM and lexical features.\nGRU+Lex+POS: A GRU model with the same features as SVM, lexical features, and POS tags.\nGR", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "surface features include morphological, syntactic, and semantic features (BIBM)\n\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface, morphological, and semantic features (IBM)\n\nAnswer:surface,", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "the modified cost (unanswerable)\n\n\nArticle: Introduction\nDistributed word representations, commonly referred to as word embeddings BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, B", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English (unanswerable\n\n\n\nArticle: Introduction \nThe new generation of Neural Machine Translation (NMT) systems is extremely data hungry BIBREF0 . Yet, most existing NMT pipelines fail to fully utilize the large amount of monolingual and parallel data that is often available. This is particularly true in the case of domain adaptation, where parallel data is often scarce. In this paper, we investigate the impact of monolingual data on NMT performance. Specifically, we focus on the following questions: 1) How important is the quality of monolingual", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "unanswerable\n\n\n\nThe framework is trained with the same hyperparameters as the original NMT system.\n\n\nAnswer:\nunanswerable\n\nQuestion: Does their framework automatically optimize for hyperparameters?\n\nAnswer:unanswerable\n\nQuestion: Does their framework automatically optimize for hyperparameters?\n\nAnswer:unanswerable\n\nQuestion: Does their framework automatically optimize for hyperparameters?\n\nAnswer:unanswerable\n\nQuestion: Does their framework automatically optimize for hyperparameters?\n\nAnswer:unanswerable\n\nQuestion: Does their framework automatically optimize for hyperparameters?\n\nAnswer", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "discreating differences between the two news domains might be country-independent but rather due to the content that is being shared (Document[111]):\"The results of our analysis suggest that differences between the two news domains might be country-independent but rather due to the content that is being shared . The results of our analysis suggest that differences between the two news domains might be country-independent but rather due to the content that is being shared .\"\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer:The main conclusion of the paper is that the topological features of multi-layer networks", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin the family of text-based games called CoinCollector BIBREF0 and the family of text-based games called CookingWorld BIBREF1 (BIBREF0, BIBREF1)\n\nAnswer:unanswerable\n\nQuestion: What is the main goal of the paper?\n\nAnswer:To propose a novel methodology for solving text-based games (BIBREF0)\n\nAnswer:unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:To propose a novel methodology for solving text-based games (BIBREF0", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F1. F1-INLINEFORM0 and F1-INLINEFORM1\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:1-INLINEFORM0\n\nQuestion: What is the main limitation of the current keyphrase generation task?\n\nAnswer:1-INLINEFORM0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:1-INLINEFORM0\n\nQuestion: What is the main limitation of the current keyphrase generation task?\n\nAnswer:1-INLINEFORM0\n\nQuestion: What is the", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "the bias in the process of collecting or annotating datasets (BIBREF22)\n\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (BREF2)\n\nAnswer: bias in the hate speech training datasets (", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "Conclusion\nWe would like to draw attention to the number of parameters used by those architectures. We note that our approach relies on a lower number of parameters (14 millions vs. 24 millions for Pudupppup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "\"In our proposed method, which is based on the GloVeVe framework, we propose to modify the objective function of the GloVeVe algorithm BIBREF10, BIBREF11, BIBREF111, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF2", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "unanswerable\n\n\n\n\"We evaluate our approach on a corpus of 351,0000 English news articles and 73,0000 English entity pages. We use the Stanford CoreNLP BIO tagger to annotate the news articles and the DBpedia entity pages. We use the DBpedia entity pages to extract the entity mentions in the news articles. We use the DBpedia entity pages to extract the entity mentions in the news articles. We use the DBpedia entity pages to extract the entity mentions in the news articles. We use the DBpedia", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "novel document- The document-level encoder based on Bert (BertSum)\n\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\nAnswer:novel\n\n", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The adaptively sparse attention weights can be used to identify the most important words in a sequence, and the attention weights can be used to identify the most important parts of a sequence.\n\nAnswer:unanswerable\n\nQuestion: How does their model improve interpretability compared to softmax transformers?\n\nAnswer:The adaptively sparse attention weights can be used to identify the most important words in a sequence, and the attention weights can be used to identify the most important parts of a sequence.\n\nAnswer:unanswerable\n\nQuestion: How does their model improve interpretability compared to softmax transformers?\n\n", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "0.5773\n\n\n\nQuestion: What is the main advantage of the morphological segmentation?\n\nAnswer:reduces the vocabulary size\n\nQuestion: What is the main advantage of the morphological segmentation?\n\nAnswer:reduces the vocabulary size\n\nQuestion: What is the main advantage of the morphological segmentation?\n\nAnswer:reduces the vocabulary size\n\nQuestion: What is the main advantage of the morphological segmentation?\n\nAnswer:reduces the vocabulary size\n\nQuestion: What is the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "22222,8888\n\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,888\n\nAnswer:22,88\n\nAnswer:22,88\n\nAnswer:22,88\n\nAnswer:22,88", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "sentence (Document)\n\n\n\nArticle:\n\nIntroduction\nAssembling training data for specialized domains such as biology, medicine, and so on is a challenging task. The reason is that the domain experts are expensive and the annotators are expensive. The annotators are expensive because they need to have the domain knowledge. The domain knowledge is expensive because it is hard to find the experts. The annotators are expensive because they need to have the domain knowledge. The domain knowledge is expensive because it is hard to find the experts. The annotators are expensive because they need to have the domain knowledge", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "personal attacks (B)\n\n\n\nAnswer:attacks (B)\n\n\nAnswer:attacks (B)\n\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\nAnswer:attacks (B)\n\n", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "We randomly sampledifferent datasets from different domains, including medical, legal, financial, and news articles (BIBM)\n\n\nQuestion: what is the purpose of the experiment?\n\nAnswer:to evaluate the performance of the proposed model on different datasets (BIBM)\n\nQuestion: what is the main conclusion of the experiment?\n\nAnswer:the proposed model performs well on different datasets (BIBM)\n\nQuestion: what is the main contribution of the experiment?\n\nAnswer:the proposed model is able to handle different types of data and achieve good performance (BIBM)\n\nQuestion", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "unanswerable\n\n\nThe model's reliability to transfer style is evaluated by human evaluation.\n\n\nAnswer:\nunanswerable\n\nQuestion: How do they evaluate the model's performance?\n\nAnswer:\nThe model's performance is evaluated by human evaluation.\n\nAnswer:\nunanswerable\n\nQuestion: How do they evaluate the model's performance?\n\nAnswer:\nThe model's performance is evaluated by human evaluation.\n\nAnswer:\nunanswerable\n\nQuestion: How do they evaluate the model's performance?\n\nAnswer:\nThe model'", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "18.2%\n\n\n\nQuestion: What is the improvement over the best performing state-of-the-art?\n\nAnswer:18.2%\n\nQuestion: What is the improvement over the best performing state-of-the-art?\n\nAnswer:18.2%\n\nQuestion: What is the improvement over the best performing state-of-the-art?\n\nAnswer:18.2%\n\nQuestion: What is the improvement over the best performing state-of-the-art?\n\nAnswer:18.2%\n\nQuestion:", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "unanswered (Document[129]):\"The human evaluation Kappa co-efficient results are calculated with respect to: DISPLAYFORM0\"\n\nAnswer:unanswerable (Document[19]):\"The No Peepholes (NP) variant of the attention mechanism is used in the RNN model . The NP variant is a variant of the attention mechanism that does not use the peephole connections . The NP variant is used in the RNN model because the peephole connections are not needed in the RNN model . The NP variant is used in the R", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Joint model (Document quality assessment (Document[11]):\"Document quality assessment (Document[1]):\"Document quality assessment (DQA) is the process of evaluating a document to determine its quality . The evaluation is based on a set of criteria, which are used to assess the document's quality . The criteria are usually based on the document's content, but can also include other factors such as the document's format, layout, and design . The evaluation is usually done by a human evaluator, but can also be done by a machine learning algorithm.\"\n\nQuestion: What", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "Ja\n\nThe most of the baselines were obtained by training the MT5 model on the original parallel corpus and testing it on the test set.\n\n\nQuestion: what was the best model?\n\nAnswer:\n\nM2M model (b3)\n\nQuestion: what was the best model for Ja-En?\n\nAnswer:\nM2M model (b3)\n\nQuestion: what was the best model for En-Ja?\n\nAnswer:\nM2M model (b3)\n\nQuestion: what was the best model for Ja-En?\n\n", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "LWide (Document[1]):\"A network features which quantify different aspects of the sharing process\"\n\n\nQuestion: What is the main goal of the paper?\n\nAnswer:Answer (Document[1]):\"In this paper we propose a classification framework based on a multi-layered representation of Twitter diffusion networks, which we call \"multi-layered diffusion networks\" (MLDNs). The main goal of this work is to investigate whether the use of multiple layers of information, which capture different aspects of the sharing process, can improve the classification performance of news articles in two different domains, namely main", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "ELMoEarXLM (Extended LM)\n\n\n\nQuestion:What is the best performing model?\n\n\nAnswer:QA-PG\n\n\nQuestion:What is the best performing model?\n\nAnswer:QA-PG\n\nQuestion:What is the best performing model?\n\nAnswer:QA-PG\n\nQuestion:What is the best performing model?\n\nAnswer:QA-PG\n\nQuestion:What is the best performing model?\n\nAnswer:QA-PG\n\nQuestion:What is the best performing model?\n\nAnswer:QA", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "relation detection (HR-BiLSTM)\n\n\n\nQuestion: What is the main focus of this paper?\n\n\nAnswer:Knowledge Base Question Answering (KBQA) systems (BIBREF0)\n\nQuestion: What is the main focus of this paper?\n\nAnswer:Knowledge Base Question Answering (KBQA) systems (BIBREF0)\n\nQuestion: What is the main focus of this paper?\n\nAnswer:Knowledge Base Question Answering (KBQA) systems (BIBREF0)\n\nQuestion: What is the", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "BIBMost (B)\n\n\nQuestion:What is the key idea of the proposed method?\n\nAnswer:The proposed method is based on the equivalence between supervised learning and RL BIBREF6, which shows that the data examples in supervised learning and the reward function in RL are equivalent. The equivalence is used to adapt an off-the-shelf reward learning algorithm in RL to the supervised setting for automated data manipulation. The manipulation is parameterized by the reward function, which is learned jointly with the model through efficient gradient updates on the validation set.\n\nQuestion", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "BioASQAggregated BioASQ (BioASQ) is a biomedical question answering challenge that is organized by the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMBL) and the European Bioinformatics Institute (EMB", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "TACMost recent summarization benchmarks have used human annotated summaries as gold standard for evaluation. The most widely used of these is the Pyramid score. The Pyramid score is a variant of the ROUGE-L score. It is based on the idea of pyramid of concepts. The idea is that the higher the level of the concept in the pyramid, the more important it is. The Pyramid score is calculated as the sum of the number of concepts in the gold summary that are at the top level of the pyramid.\n\nAnswer:unanswerable\n\n", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The word importance is calculated by the gradient of the output probability with respect to the input word embeddings (Section SECREF1).\n\nAnswer: The importance of a word is calculated by the gradient of the output probability with respect to the input word embeddings (Section SECREF1).\n\nAnswer: The importance of a word is calculated by the gradient of the output probability with respect to the input word embeddings (Section SECREF1).\n\nAnswer: The importance of a word is calculated by the gradient of the output probability with respect to the input word embeddings (Section SECREF", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "BIBMost of the art methods lack the contextual information and the commonsense knowledge of the humans have. They are based on the n-grams and other statistical features. They are not able to capture the contextual information and the commonsense knowledge of humans. They are based on the n-grams and other statistical features. They are not able to capture the contextual information and the commonsense knowledge of humans. They are based on the n-grams and other statistical features. They are not able to capture the contextual information and the commonsense knowledge of humans. They are based on the", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "significant improvement over strong (Document[1111]):\"We show that our data manipulation approach is general and can be instantiated for different manipulation schemes with the same algorithm . We demonstrate the generality by instantiating the algorithm for data augmentation and data weighting . We show that the manipulation is effective in two difficult scenarios: low data regime and class imbalance . In the low data regime, our manipulation improves the performance of the base model significantly, and outperforms the state-of-the-art methods for data augmentation and data weighting . In the class imbalance problem", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "multiple choice question answering (MCQA) (Document[24]):\"Multiple choice question answering (MCQA) is a type of machine learning task in which a computer program is given a question and a set of possible answers, and it needs to choose the best answer . The goal is to find the correct answer from the given options . The task is also known as multiple choice question answering (MCQA), multiple choice question answering (MCQA), or multiple choice question answering (MCQA).\"\n\nQuestion: What is the main focus of the article?\n\nAnswer: multiple choice question answering", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseem (Document[15]):\"We evaluate our model on two widely-studied datasets provided by Waseem et al. BIBLIORED and Davidson et al. Hatebase . BIBLIORED is a dataset of 100,0000 tweets annotated by 10 annotators and Hatebase is a dataset of 100,0000 tweets annotated by 10 annotators.\"\n\nQuestion: What is the pre-trained model used?\n\nAnswer:BERT (Document[1]):\"", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "Transformer (a) Transformer BERT (b) Transformer BERT (c) Transformer BERT (d) Transformer BERT (e) Transformer BERT (f) Transformer BERT (g) Transformer BERT (h) Transformer BERT (i) Transformer BERT (j) Transformer BERT (k) Transformer BERT (l) Transformer BERT (m) Transformer BERT (n) Transformer BERT (o) Transformer BERT (p) Transformer BERT (q) Transformer BERT (r) Transformer BERT", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "F1. \n\n\n\nAnswer:2.\n\nAnswer:3.\n\nAnswer:4.\n\nAnswer:5.\n\nAnswer:6.\n\nAnswer:7.\n\nAnswer:8.\n\nAnswer:9.\n\nAnswer:10.\n\nAnswer:11.\n\nAnswer:12.\n\nAnswer:13.\n\nAnswer:14.\n\nAnswer:15.\n\nAnswer:16.\n\nAnswer:17.\n\nAnswer:18.\n\nAnswer:19.\n\nAnswer:", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "unanswerable (Documentation of the proposed task is to evaluate the quality of the concept maps generated by the baseline method.\n\n\nAnswer:unanswerable\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:a novel task, concept map - based MDS (§ SECREF2), the creation of a new corpus of 3000 documents and 3000 concept maps (§ SECREF3), and a baseline method for the task (§ SECREF4).\n\nAnswer:unanswerable\n\nQuestion: What is the main", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "2- 2-layers regular LSTM model (RSTM)\n- 2-layers LSTM model (LSTM)\n- 3-layers LSTM model (LSTM)\n- 4-layers LSTM model (LSTM)\n- 5-layers LSTM model (LSTM)\n- 6-layers LSTM model (LSTM)\n- 7-layers LSTM model (LSTM)\n- 8-layers LSTM model (LSTM)\n- 9-layers", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "professional translation (unanswerable)\n\n\n\nQuestion: What is the main conclusion of the paper?\n\n\nAnswer:The paper concludes that both human and machine translation can introduce artifacts that affect cross-lingual transfer. (unanswerable)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:The paper contributes to the understanding of the impact of translation on cross-lingual transfer and provides insights into the behavior of cross-lingual transfer. (unanswerable)\n\nQuestion: What is the main goal of the paper?\n\nAnswer:The paper a", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "end-to-to-end MRC model (Document[10]):\"Knowledge Aided Reader (KAR) is an end-to-end MRC model.\"\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a WordNet - based data enrichment method (Document[1]):\"The main contribution of this paper is a WordNet - based data enrichment method, which is used to extract inter-word semantic connections from the text and to enrich the input passage with these connections . The resulting enriched passage is then fed into a M", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Acknowledgments\n\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Acknowledgments\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Acknowledgments\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Acknowledgments\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Acknowledgments\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:Acknowledgments\n\nQuestion: What is the main contribution of this", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "Human Level of characters with detailed information on their profiles and identities (e.g. Sheldon Cooper from The Big Bang Theory) (Document[12]):\"We propose a novel dataset called HLA, which contains detailed information on their profiles and identities (e.g. Sheldon Cooper from The Big Bang Theory) (Document[12]):\"We propose a novel dataset called HLA, which contains detailed information on their profiles and identities (e.g. Sheldon Cooper from The Big Bang Theory) (Document[12]):\"We propose a novel dataset called HLA", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "Reuters's using the Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for natural language processing tasks BIBM Reuters database, an established database for", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "Table Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "By examining the results of tweets and with respect to recently studies BIBREF22, BREF3, BREF4, BREF5, BREF6, BREF7, BREF8, BREF9, BREF10, BREF111, BREF12, BREF13, BREF14, BREF15, BREF16, BREF17, BREF18, BREF19, BREF20, BREF21, BREF22, BREF23, BREF24, BREF25", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "WebQuestions they achieve the state-of-the-art results on both single-relation and multi-relation KB relation tasks (BIBREF0).\n\nQuestion: What is the main focus of this paper?\n\nAnswer:The main focus of this paper is to improve the relation detection subtask of the knowledge base question answering (KBQA) task.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a novel relation detection model that outperforms the previous state-of-the-art models on the WebQuestions and", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "spelling\n\nunanswerable\n\n\nQuestion:\nWhat is the main advantage of their approach?\n\nAnswer:\nunanswerable\n\nQuestion:\nWhat is the main disadvantage of their approach?\nAnswer:\nunanswerable\n\nQuestion:\nWhat is the main advantage of their approach?\nAnswer:\nunanswerable\n\nQuestion:\nWhat is the main disadvantage of their approach?\nAnswer:\nunanswerable\n\nQuestion:\nWhat is the main advantage of their approach?\nAnswer:\nunanswerable\n\nQuestion:\nWhat is the main dis", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "We crawling over 2M tweets from Twitter to build a dataset with 262,7555 ironic and 10,845 non-ironic tweets (Document[11]):\"We crawl over 2M tweets from Twitter to build a dataset with 262,55 ironic and 10,845 non-ironic tweets . We use Ekphrasis to remove URLs and hashtags and Ekphrasis to remove URLs and hashtags . We use Ekphrasis to remove URLs and hashtags", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The global context of both the global (the whole document) and the local context (e.g. the local context of a sentence) when deciding if a sentence should be included in the summary (Document[6]):\"In this paper, we propose a novel approach to extracting the most important sentences from a given document, by incorporating both the global (the whole document) and the local context (e.g. the local context of a sentence) when deciding if a sentence should be included in the summary\"\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: a novel approach", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "Knowledge Base\n\n\n\n\nQuestion: What is the main focus of this paper?\n\nAnswer:Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5 . For an input question, these systems generate a KB query, which is then used to retrieve the answers from the KB. Figure 1 illustrates the process of parsing two input questions into KB queries. The first question is a single-relation", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "They applied LDA and Gibbs sampling on ISWC and WWWWW conference's publications from 2013 to 2017 (Document[111]):\"In this paper, we present a taxonomy of recommendation systems based on LDA and Gibbs sampling on ISWC and WWWW conference's publications from 2013 to 2017.\"\n\nQuestion: What is the main goal of this paper?\n\nAnswer:To present a taxonomy of recommendation systems based on LDA and Gibbs sampling on ISWC and W", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "yes (Document[The authors hypothesize that humans' robustness to noise is due to their general knowledge)\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a data enrichment method (Document[11]):\"The main contribution of this paper is a data enrichment method, which uses WordNet to extract inter-word semantic connections from passage-question pairs, and then uses these connections to enrich the passage representations . The resulting enriched passage representations are then fed into a neural network to predict the answer span.\"\n\nQuestion: What is the main idea of", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "33333% (Document[111]):\"The gender issue in media is a global issue, with a significant impact on the representation of women in the media . The Global Media Monitoring Project (GMM) is a global research project that has been monitoring the gender balance in the media since 20006 . The GMM has been implemented in 1100 countries and has collected data on the gender balance in the media in 10 languages . The GMM has found that women are underrepresented in the media in all countries . In 2015", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "unanswerable\n\n\n\n\n\nQuestion: What was the baseline model?\n\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "unanswered\n\n\n\n\n\nQuestion: What was their highest MRRank score?\n\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\nAnswer:\n\nQuestion: What was their highest MRank?\n\n", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "BLENS-19.10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "unanswerable\n\n\nThe model used in this study is a content-based classifier that uses the bag-of-words approach to represent the textual content of the blogs.\n\nAnswer:\nThe model used in this study is a content-based classifier that uses the bag-of-words approach to represent the textual content of the blogs.\n\nAnswer:\nThe model used in this study is a content-based classifier that uses the bag-of-words approach to represent the textual content of the blogs.\n\nAnswer:\nThe model used in this study is a", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Cyberlin/Politics (BIBM)\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel human-AI loop approach for keyword discovery and expectation estimation\n\nQuestion: What is the main problem that this paper addresses?\n\nAnswer:The lack of a principled approach for discovering keywords and estimating expectations\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel human-AI loop approach for keyword discovery and expectation estimation\n\nQuestion: What is the main problem that this paper addresses?\n\nAnswer:The lack of", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "We developed a RQE-based QA approach by evaluating the retrieved answers (Section SECREF0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Conditional Copy\n\n\n\n\nThe strong baseline is the best model in Wiseman's paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "the optimization challenge in our neural network comes from the intractability of the marginal likelihood (Eq. ( EQREF17) ) (BIBREF17)\n\nAnswer:The invertibility condition is a condition that we impose on the neural network to ensure that the Jacobian of the neural network is invertible. This condition is necessary to ensure that the neural network can be trained using standard optimization algorithms.\n\nAnswer:The invertibility condition is a condition that we impose on the neural network to ensure that the Jacobian of the neural network is invertible. This condition is necessary to ensure that", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Logistic Regression (BIBM) (Document[10]):\"Logistic Regression (LR) and Multilayer Perceptron (MLP) are used as the classifiers.\"\n\nQuestion: What is the dataset used?\n\nAnswer:Cyberball (Document[1]):\"Cyberball is a dataset of tweets collected from Twitter during the 2016 US presidential election . The dataset contains 10,000 tweets, which are labeled as either pro-Clinton or pro-Trump . The dataset is used to train and test the", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "INLINEFORM0\n\n\n\nQuestion: what is the size of the law articles?\n\nAnswer:INLINEFORM0\n\n\nQuestion: what is the size of the fact description?\nAnswer:INLINEFORM0\n\nQuestion: what is the size of the plea?\nAnswer:INLINEFORM0\n\nQuestion: what is the size of the result?\nAnswer:INLINEFORM0\n\nQuestion: what is the size of the input?\nAnswer:INLINEFORM0\n\nQuestion: what is the size of the output?\nAnswer:INLINEFORM0\n\nQuestion", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "MCScripts (Document[1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "Back Translation\n\n\n\n1. Back Translation\n2. Mixing\n3. Back Translation\n4. Mixing\n5. Back Translation\n6. Mixing\n7. Back Translation\n8. Mixing\n9. Back Translation\n10. Mixing\n11. Back Translation\n12. Mixing\n13. Back Translation\n14. Mixing\n15. Back Translation\n16. Mixing\n17. Back Translation\n18. Mixing\n19. Back Translation\n20. Mixing\n", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "significant (ALO)\n\n\n\nQuestion: How many characters are in the article?\n\nAnswer:10,0000\n\n\nQuestion: How many characters are in the article?\n\nAnswer:10,0000\n\nQuestion: How many characters are in the article?\n\nAnswer:10,0000\n\nQuestion: How many characters are in the article?\n\nAnswer:10,0000\n\nQuestion: How many characters are in the article?\n\nAnswer:10,0000\n\nQuestion: How", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The distinctive results show that Arabic offensive language detection is language and culture specific.\n\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:unanswerable\n\nAnswer:", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The authors show that the zero-shot performance of the Seq2Seq model trained on the trajectories found by GoExplore is much better than the zero-shot performance of the Seq2Seq model trained on the trajectories of the 4,400 games of the First Text World (FTW) dataset (see Table 1 in the main paper). (Section 5.2.1)\n\nAnswer: The authors show that their learned policy generalize better than existing solutions to unseen games by showing that the zero-shot performance of the Seq2Seq model trained on the trajectories found by GoEx", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "To investigate gender bias in ASR performance (Document[29]):\"The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to investigate the impact of gender bias in ASR performance.\"\n\nQuestion: What is the proportion of men and women in the broadcast data?\n\nAnswer: 65% men (Document[29]):\"The proportion of men and women in the broadcast data is 65% men and 35% women.\"\n\nQuestion: What is the role of the Anchor in the broadcast data?\n\nAnswer: professional speaker", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "A predictive models based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot be answered based on the above article as concisely as possible. If the question cannot", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "yes\n\n\nThe Wikipedia dataset consists of articles from English Wikipedia, with quality labels assigned by the Wikipedia community. The dataset consists of 10,0000 articles, with 1,0000 articles from each of the six quality classes: Featured Article (FA), Good Article (GA), B-class Article (B), C-class Article (C), Start Article (Start), and Stub Article (Stub).\n\nQuestion: What is the dataset used in the paper?\n\nAnswer:\nThe Wikipedia dataset consists of articles from English Wikipedia, with quality labels assigned by the Wikipedia", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "MPADA document-level pre-processing techniques, such as tokenization, stemming, and stopword removal, are applied to the text before the feature extraction step.\n\n\nQuestion: What is the state-of-the-art system?\n\nAnswer:MPAD\n\nQuestion: What is the state-of-the-art system?\n\nAnswer:MPAD\n\nQuestion: What is the state-of-the-art system?\n\nAnswer:MPAD\n\nQuestion: What is the state-of-the-art system?\n\nAnswer:MPAD\n\nQuestion", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "They propose to use the estimated word importance to detect under-translated words. Specifically, we randomly sample 5000 words from the input sentence and compute the importance of each word. Then, we randomly select 500 words with the highest importance and compute the translation quality of the original sentence with these words replaced by the under-translated words. The difference between the original and the translated sentences is the translation quality of the under-translated words. (BIBREF10)\n\nQuestion: What is the difference between the black-box and the white-box methods?\n\nAnswer", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "Wikipedia (1) an expanded version of the Conversations dataset BIBREF10, and (2) the ChangeMyView dataset BREF11 (Document[17]):\"We consider two datasets, representing related but slightly different forecasting tasks . The first is an expanded version of the Conversations dataset BIBREF0, and the second is the ChangeMyView dataset BIBREF1.\"\n\nQuestion: What is the main goal of the model?\n\nAnswer: forecasting conversations BREF1 (Document[17]):\"The main goal of the model is to forec", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "yes (unanswerable)\n\n\n\nQuestion: did they use any pre-trained model?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained model?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained model?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained model?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained model?\n\nAnswer: (unanswerable)\n\nQuestion: did", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "Europarlanguages: English, French, German, Italian, Russian, Spanish, Turkish, Chinese, Japanese, Korean, Arabic, Hebrew, Hindi, Portuguese, Polish, Thai, Vietnamese, Greek, Turkish, Persian, Bengali, Tamil, Telugu, Kannada, Malayalam, Marathi, Sanskrit, Sindhi, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sanskrit, Sansk", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "yes\n\n\"We tested UTCNN on two different datasets: FBF and CD. FBF is a privately owned, single-topic, Chinese dataset, and CD is a public, multiple-topic, English dataset. Results on these two datasets show that the proposed model is applicable to different languages and topics.\"\n\nQuestion: What is the FBF dataset?\n\nAnswer:\n\"FBFanshi Ban (FB) is a privately owned, single-topic, Chinese dataset. It is a dataset of posts from a Chinese fan page of a famous singer. The dataset is unbalanced, with 1", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "a matrix that contains the number of times a pair of words co-occurred in a corpus (Document[109]):\"A second - order co-occurrence matrix is a matrix that contains the number of times a pair of words co-occurred in a corpus . It is a generalization of the first - order co-occurrence matrix, which contains the number of times a word occurs in a corpus . The second - order co-occurrence matrix is a useful tool for analyzing the semantic relationships between words in a corpus.\"\n\nQuestion: What is the difference between a", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "precision, recall, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure, F1 - measure", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "unanswerable\n\n\nThe annotations can be accessed via the following repository: https://github.com/acohan/TAC2014-TAC2015-TAC2016-TAC2017-TAC2018-TAC2019-TAC2020-TAC2021-TAC2022-TAC2023-TAC2024-TAC2025-TAC2026-TAC2027-TAC2028-TAC2029-TAC2030-", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "by their predicted instance difficulty (Document[10]):\"We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve the performance of information extraction models . We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a random subset of the abstracts predicted to be easy . The annotation of highly technical and specialized content and the use of both lay and expert annotators allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annot", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "approx\n\n\n\nThe final dataset consists of 2,1000 texts, 14,0000 questions, and 28,0000 answer candidates.\n\n\nAnswer:\n2,100\n\nAnswer:\n2,100\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\nAnswer:\n2,10\n\n", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "yes (unanswerable)\n\n\nAnswer: yes\n\n\nQuestion: What is the vocabulary size?\n\nAnswer:unanswerable\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "ROUsed ROUsed ROGUE (BIBM) (BIBM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (BM) (", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "(1) NaiveNN (1) Naive: A non-domain-specific baseline with bag-of-words representation and SVM classifier. (2) mSD: A domain-specific baseline with bag-of-words representation and SVM classifier. (3) mSD-P: A domain-specific baseline with bag-of-words representation and SVM classifier, but with pre-trained word embeddings. (4) mSD-P-E: A domain-specific baseline with bag-of-words representation and SVM classifier, but with pre", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "590.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "word embedding techniques that use second order vectors BIBREF10\n\n\nQuestion: What is the goal of the paper?\n\nAnswer: to discover methods that automatically reduce the amount of noise in second order vectors BREF1\n\nQuestion: What is the main idea of the paper?\n\nAnswer: to integrate pairwise similarity scores into second order vectors BREF2\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: to show that integrating pairwise similarity scores into second order vectors can improve the performance of second order vectors BREF3\n\nQuestion: What is the main method of", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "US and Italy (Document[16]):\"We collected tweets associated to mainstream and disinformation news articles published on Twitter and Facebook in the US and Italy, respectively, during the period January 2019--June 2019 . We collected tweets associated to mainstream and disinformation news articles published on Twitter and Facebook in the US and Italy, respectively, during the period January 2019--June 2019 . We collected tweets associated to mainstream and disinformation news articles published on Twitter and Facebook in the US and Italy, respectively, during the period", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The Changes in the Changes in profile attributes of political handles were more than that of followers.\n\n\nQuestion: How do the users show their support to the political party through their profile attributes?\n\nAnswer:Users display their support to the political party through their profile attributes.\n\nQuestion: How do the users show their support to the political party through their profile attributes?\n\nAnswer:Users display their support to the political party through their profile attributes.\n\nQuestion: How do the users show their support to the political party through their profile attributes?\n\nAnswer:Users display their support to the political party", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "In their own human study (Table TABREF28)\n\n\n\nAnswer: their own human study (Table TABREF28)\n\n\nAnswer: their own human study (Table TABREF28)\n\nAnswer: their own human study (Table TABREF28)\n\nAnswer: their own human study (Table TABREF28)\n\nAnswer: their own human study (Table TABREF28)\n\nAnswer: their own human study (Table TABREF28)\n\nAnswer: their own human study (Table TABREF28)", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "F1. F1 - macro F1 (BIO)\n\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro F1 (BIO)\n\nAnswer:. macro", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "unanswerable\n\n\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "5K\n\n\nThe Wikipedia dataset consists of articles from English Wikipedia, with the quality class labels assigned by the Wikipedia community. The dataset consists of 1,0000 articles, with 100 articles per class.\n\n\nAnswer:\n5\n\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n\nAnswer:\n\n5\n", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "unanswerable\n\n\n\nThe translation artifacts we have identified in this paper are not specific to NLI, but rather to the general problem of training on translated data.\n\nIn the case of NLI, the artifacts we have identified are:\n\n1. The use of machine translation to translate the test set into the target language.\n2. The use of machine translation to translate the training set into the target language.\n3. The use of machine translation to translate the training set into the target language.\n4. The use of machine translation to translate the training set into the target language.\n5", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "Liability (INLINE)\n\n\n\nAnswer: LiLiLi\n\n\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\nLi\n", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Insight analysis of text as social and cultural data (Document[1]):\"Text analysis is the process of extracting meaningful information from text . It is a collection of techniques and tools that help researchers to understand the content of text data . Text analysis is a branch of natural language processing, which is concerned with the computer - based manipulation of natural language . Text analysis is a sub-discipline of the field of linguistics, which is concerned with the scientific study of language . Text analysis is also a sub-discipline of the field of computer science, which is concerned with the scientific study of computation and", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Reddit does not mentioned (Document[10]):\"The dataset is not mentioned in the article.\"\n\n\nQuestion: What is the main question of the article?\n\nAnswer:Does eliminating hate speech from online platforms reduce hate speech in the real world? (Document[10]):\"The article discusses the question of whether eliminating hate speech from online platforms reduces hate speech in the real world . The article does not provide a specific dataset for analysis.\"\n\nQuestion: What is the main conclusion of the article?\n\nAnswer:The article does not provide a specific dataset for analysis . (Document", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Energy (Energy)\n\n\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer:Energy\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer:Energy\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer:Energy\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:Energy", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The authors have diverse backgrounds in the humanities and social sciences (BIBM, LS, MH, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT, MT", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "SVM\n\n\n\nWe experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and investigate their performance in in-domain and cross-domain scenarios.\n\nWe use the following machine learning methods:\n\n1. Naive Bayes\n2. Decision Trees\n3. Random Forest\n4. Support Vector Machines\n5. Neural Networks\n6. Long Short-Term Memory (LSTM)\n7. Recurrent Neural Networks (RNN)\n8. Convolutional Neural Networks (", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "Conversatile framework for the norms for MPCS\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a framework for the norms for MPCS\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a framework for the norms for MPCS\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a framework for the norms for MPCS\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a framework for the norms for MPCS\n\nQuestion: What is the", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "a corpusan annual dataset of state speeches and votes on the United Nations General Assembly (UNGA) and Security Council (UN SC) resolutions from 1947 to 2013 (BIBREF1)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a new approach to estimate preference polarization in multidimensional settings using texts and votes (BIBREF1)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a new approach to estimate preference polarization in multidimensional settings using texts and votes (BIB", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Doc2. \n\n\n\n\nQuestion: What is the purpose of the text embedding methodologies?\n\nAnswer:3.\n\n\nQuestion: What is the purpose of the text embedding methodologies?\nAnswer:3.\n\nQuestion: What is the purpose of the text embedding methodologies?\nAnswer:3.\n\nQuestion: What is the purpose of the text embedding methodologies?\nAnswer:3.\n\nQuestion: What is the purpose of the text embedding methodologies?\nAnswer:3.\n\nQuestion: What is the purpose of the text embedding methodologies?\nAnswer:3", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "argument components (1) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate (IN) Claimmediate (OUT) Claimmediate", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "yes (unanswerable)\n\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?\n\nAnswer: yes\n\nQuestion: Do they report results only on English data?", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "unanswerable\n\n\n\nThe datasets used in this paper are the following:\n\n\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n10.\n111.\n12.\n13.\n14.\n15.\n16.\n17.\n18.\n19.\n20.\n21.\n22.\n23.\n24.\n25.\n26.\n27.\n28.\n29.\n", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "unanswerable\n\n\nThe proposed framework for analyzing dehumanizing language in this paper is based on the following components of dehumanization:\n\n1. Negative Evaluation\n2. Moral Disgust\n3. Denial of Agency\n4. Denial of Moral Agency\n5. Denial of Moral Agency\n6. Denial of Moral Agency\n7. Denial of Moral Agency\n8. Denial of Moral Agency\n9. Denial of Moral Agency\n10. Denial of Moral Agency\n11. Denial of Moral Agency", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "a state\n\n\n\n\nA node in a network approach represents a state or a country.\n\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the purpose of the network approach?\n\nAnswer:\n\nAnswer:\n\nThe purpose of the network approach is to represent a state or a country.\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the relationship between the nodes in the network?\n\nAnswer:\n\nAnswer:\n\nThe relationship between the nodes in the network is that they are connected to each other.\n\nAnswer:\n\nun", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. (Document[109]):\"We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date . The corpus consists of 3,000 documents from 10 different domains, including blogs, forums, and news articles, and covers a wide range of controversial topics, such as education, politics, and religion .", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "We used the New York Times's own API to retrieve all articles published in the New York Times from 1981 to 2019, and then manually annotated the articles to identify those that discuss LGBTQ people (BIBREFSECREF0).\n\nQuestion: What is the primary goal of this study?\n\nAnswer:to develop a computational framework for analyzing dehumanizing language in the media (BIBREFSECREF0).\n\nQuestion: What is the focus of this study?\n\nAnswer:dehumanizing language in the media (BIBREFSE", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "yes\n\n\n\n\"Interdisciplinary insights and partnerships are essential for realizing the full potential of any computational text analysis that involves social and cultural concepts. The more we can bridge the gap between the humanities and the social sciences on the one hand, and the computer sciences on the other, the more we can realize the full potential of computational text analysis.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "thornate of the intersection of “thick” cultural and societal questions on the one hand, and the analysis of rich textual data on a larger scale on the other (Bingley, 2017, p. 1)\n\n\nQuestion: What are the goals of the article?\n\nAnswer:to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods (Bingley, 2017, p. 1)\n\nQuestion: What are the three goals of the article?\n\nAnswer:to shed light on th", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "accuracy (Table UIDREF!1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "yes\n\n\"The most of the literature on the study of preferences in the international system has been based on the analysis of the votes cast by the states in the UN General Assembly. The votes are a good indicator of the preferences of the states, but they are not the only indicator of the preferences. The votes are a good indicator of the preferences of the states, but they are not the only indicator of the preferences. The votes are a good indicator of the preferences of the states, but they are not the only indicator of the preferences. The votes are a good indicator of the preferences of the", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "\"First, not all the related works are tightly connected to argumentation theories and computational linguistics, resulting into a gap between the substantial research in argumentation and the actual application of argumentation in NLP. Second, the annotation of argumentation in user-generated content is a challenging task, as the annotators have to deal with the ambiguity of the text and the lack of a clear annotation scheme. Third, the annotation of argumentation in user-generated content is a challenging task, as the annotators have to deal with the ambiguity of the text and the lack of a clear annotation scheme.\"\n\nAnswer", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "manually annotated (Document[10]\n\n\n\nQuestion: What is the focus of the article?\n\nAnswer: lexical similarity (Document[16]):\"The focus of the article is lexical similarity, which is the degree to which words have the same meaning . The article discusses the concept of lexical similarity and its importance in machine translation, and presents a method for measuring lexical similarity based on the distribution of word co-occurrences in text . The method is based on the idea that words that are used in similar contexts are likely to have similar meanings . The article also", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "figurative phenomena that are now accounted for by this work?\n\n\nAnswer:\nunanswerable\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:\n\nunanswerable\n\n\nQuestion: What is the main", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "Welshu\n\n\n\n1. English\n2. French\n3. German\n4. Italian\n5. Spanish\n6. Russian\n7. Mandarin Chinese\n8. Japanese\n9. Korean\n10. Portuguese\n11. Arabic\n12. Welsh\n\n\nQuestion: What is the focus of the article?\n\nAnswer:\n\nThe article discusses the creation of a large-scale dataset of word similarity judgments for 12 languages, with the goal of facilitating research in natural language processing and machine learning. The dataset includes word similarity judgments", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}

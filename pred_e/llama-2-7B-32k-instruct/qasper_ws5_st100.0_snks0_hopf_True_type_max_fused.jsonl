{"pred": "Fox News Corporation (Foxy)\n\n\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: Affective\n\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: Affective\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: Affective\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: Affective\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: Affective\n\n", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "linear context INLINEFORM0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-\n\n\n\nThe classification accuracy of the system is evaluated using the macro-average F1 score.\n\n\nThe macro-average F1 score is calculated as follows:\n\n\nF1 score = 2 × (precision ∗ recall) / (precision + recall)\n\nprecision = TP / (TP + FN)\n\nrecall = TN / (FP + TN)\n\nprecision and recall are computed separately for each class.\n\nThe macro-average F1 score is the average of the precision and recall scores of all classes.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "plain LSTA stacked LSTM\n\n\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:What was the best model?\n\nAnswer:CAS\n\nQuestion:", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "unanswerable (jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:jiant:ji", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "yes (unanswerable\n\n\n\nQuestion: What is the size of the dataset?\n\nAnswer:3,0000\n\nQuestion: What is the size of the dataset?\nAnswer:3,0000\n\nQuestion: What is the size of the dataset?\nAnswer:3,0000\n\nQuestion: What is the size of the dataset?\nAnswer:3,0000\n\nQuestion: What is the size of the dataset?\nAnswer:3,000\n\nQuestion: What is the size of the dataset?\nAnswer:3", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "unanswerable\n\n\nThe results in Table TABREF12 show that shallow-coding does not improve the performance of the sentiment classification task.\n\n\nQuestion: Does this method help in NER task improvement?\n\nAnswer:\nThe results in Table TABREF1 show that shallow-coding does not improve the performance of the NER task.\n\n\nQuestion: Does this method help in POS tagging task improvement?\n\nAnswer:\nThe results in Table TABREF2 show that shallow-coding does not improve the performance of the POS tagging task", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon reviews from Amazon (Amazon) and Yelpıcakılar (Yelpıcakılar)\n\nAnswer: Amazon (Amazon) and Yelpıcakılar (Yelpıcakılar)\n\nAnswer: Amazon (Amazon) and Yelpıcakılar (Yelpıcakılar)\n\nAnswer: Amazon (Amazon) and Yelpıcakılar (Yelpıcakılar)\n\nAnswer: Amazon (Amazon) and Yel", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "12222.27%\n\n\n\nAnswer:2.27%\n\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.27%\n\nAnswer:2.2", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "transcribed (unanswerable)\n\n\nThe dataset used in this paper is the IEMO 2014 dataset, which includes audio and text files.\n\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer:unanswerable\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer:unanswerable\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer:unanswerable\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer:", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUT\n\n\n\n1. CLUTER:\n2. CLUTER:\n3. CLUTER:\n4. CLUTER:\n5. CLUTER:\n6. CLUTER:\n7. CLUTER:\n8. CLUTER:\n9. CLUTER:\n10. CLUTER:\n11. CLUTER:\n12. CLUTER:\n13. CLUTER:\n14. CLUTER:\n15. CLUTER:\n16. CLUTER:\n17.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT\n\n\nBERT5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "pivoting\n\n\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting\nPivoting", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "EmotionLines 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "several evaluation protocols for each part of the database.\n\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to describe the final version of the database.\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to describe the final version of the database.\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to describe the final version of the database.\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to describe the final version of the database.\n\nQuestion: what is the main goal of the paper", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian mask queries, keys, queries, keys, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries, queries,", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "unanswerable\n\n\nThe system uses parallel data of the source language and the target language.\n\n\nAnswer:\nunanswerable\n\nQuestion: How much training data is used by the system?\n\nAnswer:\nThe system uses parallel data of the source language and the target language.\n\nAnswer:\nunanswerable\n\nQuestion: How much training data is used by the system?\n\nAnswer:\nThe system uses parallel data of the source language and the target language.\n\nAnswer:\nunanswerable\n\nQuestion: How much training data is used by the system?\n\nAnswer", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "unanswered that the attention does not always comply with alignments (BIBM)\n\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:alignments (BM)\n\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:alignments (BM)\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:alignments (BM)\n\nQuestion: What is the difference between attention and alignment?\n\nAnswer:alignments (BM)\n\nQuestion: What is the difference between attention and alignment?\n\n", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "RNN (RNN)\n\n\n\nQuestion: What is the main question of the paper?\n\nAnswer:How to reduce the cost of training a large model\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer:Efficiently training a large model is possible\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:A new method for training a large model\n\nQuestion: What is the main idea of the paper?\n\nAnswer:A new method for training a large model\n\nQuestion: What is the main result of the paper?\n\nAnswer:", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "Bi-\n\n\nBi-directional LSTM (BioNet)\n\n\n\nAnswer:\nBi-directional LSTM (BioNet)\n\n\nAnswer:\nBi-directional LSTM (BioNet)\n\nAnswer:\nBi-directional LSTM (BioNet)\n\nAnswer:\nBi-directional LSTM (BioNet)\n\nAnswer:\nBi-directional LSTM (BioNet)\n\nAnswer:\nBi-directional LSTM (BioNet)\n\nAnswer:\nBi", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "20202020\n\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:1. Propose a novel model based on pre-trained language model BERT.\n2. Propose a novel model based on pre-trained language model BERT.\n3. Propose a novel model based on pre-trained language model BERT.\n4. Propose a novel model based on pre-trained language model BERT.\n5. Propose a novel model based on pre-trained language model BERT.\n6. Propose a", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "50 (Document[50]):\"We used 111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "SVM (1) linear SVM (SVM) (2) Convolutional Neural Network (CNN) (3) Bidirectional LSTM (BLSTM) (4) Bi-directional LSTM (BLSTM) (5) Convolutional Neural Network (CNN) (6) Long Short Term Memory (LSTM) (7) Recurrent Neural Network (RNN) (8) Recurrent Neural Network (RNN) (9) Recurrent Neural Network (RNN) (10) Recurrent Neural Network (RNN", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Bi- Bi- LSTMi-DNN (Bidirectional LSTM-Bi-directional LSTM)\n-CNN (Convolutional Neural Network)\n-RNN (Recurrent Neural Network)\n-Bi-directional LSTM (Bidirectional LSTM)\n-CRNN (Convolutional Recurrent Neural Network)\n-GRU (Gated Recurrent Unit)\n-GRU (Gated Recurrent Unit)\n-GRU (Gated Recurrent Unit)\n-GRU (Gated Recurrent Unit)\n", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "UThrough the acoustic-to-phoneme (DT)\n\n\nQuestion:What is the difference between UTD and UT?\n\nAnswer:Ut (DT)\n\nQuestion:What is the difference between UT and UTD?\n\nAnswer:Ut (DT)\n\nQuestion:What is the difference between UT and UTD?\n\nAnswer:Ut (DT)\n\nQuestion:What is the difference between UT and UTD?\n\nAnswer:Ut (DT)\n\nQuestion:What is the difference between UT and UTD?", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERTREFERN/A\n\n\n\nQuestion:What is the main contribution of this paper?\n\nAnswer:The proposed method is evaluated on the NLI datasets MNLI, QQA, and XNLI, and the QQP dataset. The results show that the proposed method outperforms the baseline model on all the datasets.\n\nAnswer:The proposed method is evaluated on the NLI datasets MNLI, QNLI, XNLI, and the QQP dataset. The results show that the proposed method outperforms the baseline model on all the datasets.\n", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase five keyphrase", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "yes\n\n\n\nIn this paper, we evaluate the effectiveness of pre-training in the encoder-decoder framework. We train the encoder-decoder framework with the pre-training and compare it with the encoder-decoder framework without pre-training. The encoder-decoder framework with pre-training achieves better performance on English-Japanese translation task.\n\nAnswer:\nunanswerable\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\nThe main contribution of this paper is to propose a framework that can be trained without pre-training.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIMDBLPJournal of the field of machine learning, 2018; 28(1): 1--11\n\n\nQuestion:What is the upper bound on the number of words in the discharge summaries?\n\nAnswer:11\n\nQuestion:What is the upper bound on the number of words in the discharge summaries?\n\nAnswer:11\n\nQuestion:What is the upper bound on the number of words in the discharge summaries?\n\nAnswer:11\n\nQuestion:What is the upper bound on the number of words in the", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "14,14141\n\n\nOLID: 141\n\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID: 141\nOLID:", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "the relevance of words in the source language (Document[111]):\"Attention is a mechanism that allows a model to focus on a specific part of the input sequence . It is a mechanism that allows a model to focus on a specific part of the input sequence . It is a mechanism that allows a model to focus on a specific part of the input sequence . It is a mechanism that allows a model to focus on a specific part of the input sequence . It is a mechanism that allows a model to focus on a specific part of the input sequence . It is a mechanism that allows a model to focus on a specific part", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "BOW (1) Logistic Regression (LR) (2) Random Forest (3) Support Vector Machine (SVM) (4) TextCNN (5) Bidirectional Encoder (6) Long Short Term Memory (LSTM) (7) Recurrent Neural Network (RNN) (8) Convolutional Neural Network (CNN) (9) Word2Vec (10) GloVe (11) Word2Vec (12) GloVe (13) Word2Vec (14) GloVe (15) Word2Vec (", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "19 (Document[INST]\n\n\n\nQuestion: what is the dataset?\n\nAnswer:219,0000 e-books (Document[11]):\"The dataset contains 219,000 e-books from the Project Gutenberg collection . The books are encoded in plain text format and are available in the original language . The dataset is split into training and test sets with a ratio of 8:2.\"\n\nQuestion: what is the dataset used for?\n\nAnswer:training and test sets (Document[11]):\"The dataset is", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "end-to-to-end models, which are trained separately for each language pair.\n\n\nAnswer:unanswerable\n\nQuestion: What is the architecture of their model?\n\nAnswer:end-to-end models, which are trained separately for each language pair.\n\nAnswer:unanswerable\n\nQuestion: What is the architecture of their model?\n\nAnswer:end-to-end models, which are trained separately for each language pair.\n\nAnswer:unanswerable\n\nQuestion: What is the architecture of their model?\n\nAnswer:end-to-end models, which", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Pennui (unanswerable)\n\n\nQuestion:what is the difference between the pyramid model and the LSTM model?\n\nAnswer:The difference between the Pyramid model and the LSTM model is that the Pyramid model uses a hierarchical attention mechanism, while the LSTM model uses a linear transformation. (unanswerable)\n\nQuestion:what is the difference between the Pyramid model and the LSTM model?\n\nAnswer:The difference between the Pyramid model and the LSTM model is that the Pyramid model uses a hier", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "unanswerable\n\n\n\n\nQuestion: Do they use clustering?\n\nAnswer:answerable\n\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable\n\nQuestion: Do they use SVM?\nAnswer:answerable", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "\"Since the student model's vocabstraction is to use the teacher's vocab to generate the student's output, the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student's vocab is the student", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "word20\n\n\n\nQuestion:1 / 2\n\nAnswer:word2vec\n\nQuestion:3 / 4\nAnswer:\nword2vec\nQuestion:5 / 6\nAnswer:\nword2vec\nQuestion:7 / 8\nAnswer:\nword2vec\nQuestion:9 / 10\nAnswer:\nword2vec\nQuestion:11 / 12\nAnswer:\nword2vec\nQuestion:13 / 14\nAnswer:\nword2vec\nQuestion:15 / 16\nAnswer:\nword2vec\nQuestion", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "1. To build the internet (Document[17]):\"Data Collection . To build the dataset, we crawled 1.7K ancient Chinese articles from the internet . The articles were collected from the following websites: Baidu, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao, Baidu Zhidao,", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "unanswerable\n\n\n\nIntroduction\nThe answerability of a question is a measure of how likely a question is to be answered. The answerability of a question is a function of the quality of the question and the quality of the answers. The quality of the question is a function of the clarity of the question, the relevance of the question, and the expertise of the questioner. The quality of the answers is a function of the relevance of the answers, the expertise of the answerer, and the quality of the answer.\n\nThe answerability of a question is a measure of how likely a", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "words embeddifferent features (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-level features) (chunking) (Document-", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "XN/\n\n\n\nWe evaluate our models on two zero-shot cross-lingual transfer tasks: cross-lingual natural language inference (XNLI) and cross-lingual sentence classification (XSC).\n\nXNLI:\nWe use the XNLI dataset BIBREF18, which consists of 100K sentence pairs from 10 languages.\n\nXSC:\nWe use the XSC dataset BIBREF18, which consists of 100K sentence pairs from 10 languages.\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable\n\nThe paper focuses on English, but the method can be applied to any language.\n\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:\nThe main contribution of the paper is the incorporation of subword information into the counting matrix.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:\nThe main contribution of the paper is the incorporation of subword information into the counting matrix.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:\nThe main contribution of the paper is the incorporation of subword", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "unanswerable\n\n\n\nThe dataset is not balanced between speakers of different Languages.\n\n\nAnswer:\nunanswerable\n\nAnswer:\n\nThe dataset is not balanced between speakers of different Languages.\n\nAnswer:\n\nThe dataset is not balanced between speakers of different Languages.\n\nAnswer:\n\nThe dataset is not balanced between speakers of different Languages.\n\nAnswer:\n\nThe dataset is not balanced between speakers of different Languages.\n\nAnswer:\n\nThe dataset is not balanced between speakers of different L", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "over 450000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "ELMoSeveral (Document[10]):\"We conducted a comprehensive comparison with the baseline models, some of which have never been applied in this task, including traditional TF-IDF, Doc2Vec, Doc2Vec with sub-sampling, Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling and Doc2Vec with sub-sampling", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "25148 (Document[10]):\"The dataset consists of 14 subjects, each recorded for 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Retrieval\n\n\n1. Retrieval\n2. Search\n3. Question Answering\n4. Conversation\n5. Conversation\n6. Conversation\n7. Conversation\n8. Conversation\n9. Conversation\n10. Conversation\n11. Conversation\n12. Conversation\n13. Conversation\n14. Conversation\n15. Conversation\n16. Conversation\n17. Conversation\n18. Conversation\n19. Conversation\n20", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "yes (unanswerable)\n\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n\nAnswer: yes\n", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable (unanswerable\n\n\nThe corpus contains speech from speakers with different accents.\n\n\nQuestion: what languages are present in the corpus?\n\nAnswer:unanswerable\nThe corpus contains speech from speakers with different languages.\n\nQuestion: what dialects are present in the corpus?\n\nAnswer:unanswerable\nThe corpus contains speech from speakers with different dialects.\n\nQuestion: what gender is present in the corpus?\n\nAnswer:unanswerable\nThe corpus contains speech from speakers of different genders.\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "SQuestions are performed on the SQuAD dataset (SQuAD) BIB (Bib)\n\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:The main contribution of the paper is to propose a novel framework for question generation from reading comprehension (QGRC) and to show that the proposed framework can achieve state-of-the-art performance on the SQuAD dataset (SQuAD) and the BIB dataset (Bib)\n\nQuestion:What is the main focus of the paper?\n\nAnswer:The main focus of the paper is to propose a", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "SemEarX\n\n\n\nQuestion:What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?\n\nAnswer:\n\nQuestion: What is the best CNN architecture for relation classification?", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "\n\nThe training corpora concatenation of the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2017 (English Wikipedia is about 2.5 times larger than Russian Wikipedia).\n\n\nAnswer:\nunanswerable\n\n\nQuestion: What was the size of the corpora they used?\n\nAnswer:\na concatenation of the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2017 (English Wikipedia is about 2.5 times larger than Russian Wikipedia).\n\n\n\n\n\n\n\n", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "GMini\n\n\nWe train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train our model on the Text8 corpus and evaluate on the SCWS dataset.\nWe also train", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "significant performance boost (unanswerable)\n\n\nQuestion: What is the main idea of the paper?\n\nAnswer: alleviate the severe data imbalance issue (unanswerable)\n\nQuestion: What is the main idea of the paper?\n\nAnswer: alleviate the severe data imbalance issue (unanswerable)\n\nQuestion: What is the main idea of the paper?\n\nAnswer: alleviate the severe data imbalance issue (unanswerable)\n\nQuestion: What is the main idea of the paper?\n\nAnswer: alleviate the severe data", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "\n\n\nThe authors calculate the variance of the model outputs by computing the difference between the model outputs and the true labels. They then take the square root of this value to obtain the variance.\n\nAnswer:\nunanswerable\n\nQuestion: How do they calculate the gradient of the model outputs?\n\nAnswer:\nThe authors calculate the gradient of the model outputs by computing the partial derivative of the loss function with respect to the model outputs.\n\nAnswer:\nunanswerable\n\nQuestion: How do they calculate the expected value of the model outputs?\n\nAnswer:\nThe authors calculate the expected value of", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "LSTMost of the available context\n\n\n\nQuestion:What is the main idea behind the system?\n\nAnswer:to provide the encoder with the entire context\n\nQuestion:What is the main idea behind the system?\n\nAnswer:to provide the encoder with the entire context\n\nQuestion:What is the main idea behind the system?\n\nAnswer:to provide the encoder with the entire context\n\nQuestion:What is the main idea behind the system?\n\nAnswer:to provide the encoder with the entire context\n\nQuestion:What is the main idea behind the system?\n", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "natural language understanding of the sentiment of the sentiment (Document[111]):\"The dataset contains 500,0000 questions and 100,0000 answers . The questions are divided into 8 different categories: 1) sentiment analysis (Document[11]):\"The dataset contains 50,000 questions and 100,000 answers . The questions are divided into 8 different categories: 1) sentiment analysis 2) aspect based sentiment analysis 3) opinion mining 4) aspect based opinion mining 5) em", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes (BERT)\n\n\n\nQuestion: What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to the question \"What is the answer to", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "533 (Document)\n\n\n\nQuestion:How many sentences are in the corpus?\n\nAnswer:16,7700 (Sentence)\n\nQuestion:How many words are in the corpus?\n\nAnswer:16,700 (Sentence)\n\nQuestion:How many tokens are in the corpus?\n\nAnswer:16,70 (Sentence)\n\nQuestion:How many sentences are in the corpus?\n\nAnswer:16,70 (Sentence)\n\nQuestion:How many tokens are", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food.com (20)\n\n\n\nQuestion: What is the name of the dataset they use?\n\nAnswer:Food.com (2)\n\nQuestion: How many recipes are in the dataset?\n\nAnswer:180,0000 (2)\n\nQuestion: How many users are in the dataset?\n\nAnswer:180,0000 (2)\n\nQuestion: How many reviews are in the dataset?\n\nAnswer:180,0000 (2)\n\nQuestion: How many ingredients are in the", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "tab:intrough we evaluate the resulting word embeddings using the WordSim353 metric, which is a variant of the WordSim353 metric used in BIBREF13 . The WordSim353 metric is a variant of the WordSim353 metric used in BIBREF13 . The WordSim353 metric is a variant of the WordSim353 metric used in BIBREF13 . The WordSim353 metric is a variant of the WordSim353 metric used in BIBREF13 . The WordSim353 metric is", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "unanswerable\n\n\n10: None\n1: 13: 14: 15: 16: 17: 18: 19: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "150\n\n\n\n\nQuestion: How many texts are there in the dataset?\n\nAnswer:1,868\n\n\nQuestion: What is the average length of the texts?\n\nAnswer:300\n\nQuestion: What is the total number of tokens in the dataset?\n\nAnswer:386,800\n\nQuestion: What is the number of texts for each language?\n\nAnswer:1,868\n\nQuestion: What is the number of tokens for each language?\n\nAnswer:386,800\n\nQuestion:", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "textual patterns learned from an annotated corrections (BIBR: annotated corrections) (SECREF0)\n\nQuestion: What is the purpose of the error generation?\n\nAnswer:to create artificial errors (BIBR: artificial errors) (SECREF0)\n\nQuestion: What is the main goal of the paper?\n\nAnswer:to investigate two supervised error generation methods (BIBR: two supervised error generation methods) (SECREF0)\n\nQuestion: What is the main focus of the paper?\n\nAnswer:two supervised error generation methods (", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "unanswerable\n\n\nThe concatenation of the two vectors is done by concatenating the two vectors along the same axis.\n\n\nAnswer:\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "RKS approach proposed in BIBM (Random Kitchen Sink) is a simple and effective approach to improve the performance of any machine learning model. It is based on the idea of randomly combining different machine learning algorithms and parameters to create a new model. The new model is then trained on the same dataset as the original model, and its performance is evaluated. The process is repeated multiple times, and the best performing model is selected.\n\nThe RKS approach is a simple and effective approach to improve the performance of any machine learning model. It is based on the idea of randomly combining different machine learning algorithms and parameters to create a", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "a Big BERT (BIBM)\n\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: ELMo\n\nQuestion: What is the best performing strategy?\n\nAnswer: EL", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The results are improved by making use of artificial data\n\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The paper concludes that the use of artificial data is a good way to improve the performance of error correction systems.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The paper proposes a new method for generating artificial data that can be used to improve the performance of error correction systems.\n\nQuestion: What is the main method of the paper?\n\nAnswer: The paper proposes a new method for generating artificial data that can be used to improve the performance", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "none (unanswerable)\n\n\nQuestion: What is the name of the dataset they use?\n\nAnswer: Twitter (unanswerable)\n\nQuestion: What is the name of the model they use?\n\nAnswer: tweet2vec (unanswerable)\n\nQuestion: What is the name of the paper they are presenting?\n\nAnswer: tweet2vec: learning vector embeddings for tweets (unanswerable)\n\nQuestion: What is the name of the paper they are presenting?\n\nAnswer: tweet2vec: learning vector embeddings for twe", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "seven experts with legal training (Document[17]):\"We recruit seven experts with legal training to annotate the questions and answers.\"\n\n\nAnswer:seven (Document[7]):\"We recruit seven experts with legal training to annotate the questions and answers.\"\n\n\nAnswer:seven (Document[7]):\"We recruit seven experts with legal training to annotate the questions and answers.\"\n\nAnswer:seven (Document[7]):\"We recruit seven experts with legal training to annotate the questions and answers.\"\n\nAnswer:se", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "machine learning-based approaches (BIBM, CRF, NNLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others) (BM, CR, NLP, and others)", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford'Stan evaluation of errors on the automatically generated named entity recognition dataset (ENRD) BIBREF10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN/\n\n\n\nWe use two popular knowledge bases: WordNet BM25018 and WordNet BM2018.\n\nThe WordNet BM2018 is a large-scale knowledge base, which contains 1018 million entities and 1018 million relations.\n\nThe WordNet BM2018 is a large-scale knowledge base, which contains 1018 million entities and 1018 million relations.\n\nThe WordNet BM2018 is a large-scale knowledge base, which contains", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "LastState-of-the-the-art (BIBMREF0)\n\n\nQuestion: What is the attention mechanism?\n\nAnswer:classification (BMREF0)\n\nQuestion: What is the attention mechanism?\n\nAnswer:classification (BMREF0)\n\nQuestion: What is the attention mechanism?\n\nAnswer:classification (BMREF0)\n\nQuestion: What is the attention mechanism?\n\nAnswer:classification (BMREF0)\n\nQuestion: What is the attention mechanism?\n\nAnswer:classification (BMREF0)", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "three (Formeradicacy)\n\n\n\nQuestion: What is the main conclusion of the article?\n\nAnswer:DNN based models outperform traditional machine learning models for cyberbullying detection.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:DNN based models outperform traditional machine learning models for cyberbullying detection.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:DNN based models outperform traditional machine learning models for cyberbullying detection.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "yes\n\n\n\"In this paper, we focus on English language data. We use the ISIS corpus BIBREF10, which contains 10000 English language articles from the internet. The corpus is divided into 10000 documents, each of which contains 100000 words. The corpus is divided into 10000 documents, each of which contains 100000 words. The corpus is divided into 10000 documents, each of which contains 100000 words. The corpus", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "an annotated Twitter dataset that was constructed based on a hierarchical depression schema BIBREF12 . The dataset contains 9,473 annotations for 9,473 tweets . Each tweet is annotated as no evidence of depression or evidence of depression . If the tweet is annotated as evidence of depression, then the tweet is annotated as either major depression or minor depression . If the tweet is annotated as no evidence of depression, then the tweet is annotated as either no major depression or no minor depression .\n\nAnswer:", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "Nguni (zul)\n\n\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer:Sotho\n\nQuestion: Which languages are similar to each other?\n\nAnswer:Sotho\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer:Nguni\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer:Nguni\n\nQuestion: Which languages are similar to each other?\n\nAnswer:Sotho\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer:Nguni\n\n", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IMDb dataset of movie reviews (BIBMREF1)\n\n\nQuestion: What is the optimal window size for the skipgram model?\n\nAnswer: 10 (BIBMREF2)\n\nQuestion: What is the optimal learning rate for the skipgram model?\n\nAnswer: 0.001 (BIBMREF2)\n\nQuestion: What is the optimal number of negative samples for the negative sampling method?\n\nAnswer: 10 (BIBMREF2)\n\nQuestion: What is the optimal batch size for the negative sampling method?\n\nAnswer:", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "8999.6%\n\n\n\n\nQuestion: What was the accuracy of the system?\n\nAnswer:8.6%\n\n\nQuestion: What was the precision of the system?\n\nAnswer:8.6%\n\n\nQuestion: What was the recall of the system?\n\nAnswer:9.6%\n\n\nQuestion: What was the F1 score of the system?\n\nAnswer:8.6%\n\n\nQuestion: What was the F2 score of the system?\n\nAnswer:8.6%\n\n\nQuestion: What was the F", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K- K- K-means\n\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a novel approach based on adversarial learning for event extraction from text.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a novel approach based on adversarial learning for event extraction from text.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a novel approach based on adversarial learning for event", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English (Edinburgh)\n\n\n\nQuestion: What is the name of the system that is used for restaurant search and booking?\n\nAnswer:PolyResponse\n\nQuestion: What is the name of the system that is used for restaurant search and booking?\n\nAnswer:PolyResponse\n\nQuestion: What is the name of the system that is used for restaurant search and booking?\n\nAnswer:PolyResponse\n\nQuestion: What is the name of the system that is used for restaurant search and booking?\n\nAnswer:PolyResponse\n\nQuestion: What is the name of", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "Table TABSTREREF10\n\n\nQuestion:What is the purpose of the system?\n\nAnswer:STREF1\n\nQuestion:What is the type of the system?\nAnswer:STREF2\n\nQuestion:What is the type of the dataset?\nAnswer:STREF3\n\nQuestion:What is the purpose of the paper?\nAnswer:STREF4\n\nQuestion:What is the type of the article?\nAnswer:STREF5\n\nQuestion:What is the type of the question?\nAnswer:STREF6\n\nQuestion:", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "unanswerable\n\n\nThe lexicon is the same for all languages\n\n\nAnswer:\nunanswerable\n\nQuestion: Is the lexicon the same for all languages?\n\nAnswer:unanswerable\n\nQuestion: Is the lexicon the same for all languages?\n\nAnswer:unanswerable\n\nQuestion: Is the lexicon the same for all languages?\n\nAnswer:unanswerable\n\nQuestion: Is the lexicon the same for all languages?\n\nAnswer:unanswerable\n\nQuestion: Is the lexicon the same for all languages?\n\nAnswer:unanswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "average GloVan average GloVe GloVe\n\n\nAnswer:unanswerable\n\n\nQuestion: What is the performance of SBERT?\n\nAnswer:12.5\n\nAnswer:unanswerable\n\nQuestion: What is the performance of InferSentence?\n\nAnswer:12.5\n\nAnswer:unanswerable\n\nQuestion: What is the performance of Universal Sentence Representation?\n\nAnswer:12.5\n\nAnswer:unanswerable\n\nQuestion: What is the performance of Universal Sentence Representation?\n\nAnswer:1", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Sentence- Semanticip-BERT5-STSentiment14-STSentiment5-STSentiment6-STSentiment7-STSentiment8-STSentiment9-STSentiment10-STSentiment11-STSentiment12-STSentiment13-STSentiment14-STSentiment15-STSentiment16-STSentiment17-STSentiment18-STSentiment19-STSentiment19-STSent", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "30.5 million (Document[19]):\"The vocabbreviation is a list of 5 million words, which is used to represent the vocabulary of the model . The vocabulary is a list of words that are used to represent the knowledge of the model . The vocabulary is a list of words that are used to represent the knowledge of the model . The vocabulary is a list of words that are used to represent the knowledge of the model . The vocabulary is a list of words that are used to represent the knowledge of the model . The voc", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Contextualized tweets\n\n\n\nQuestion:What is the conclusion of the article?\n\nAnswer:The use of contextualized tweets improves the accuracy of the model.\n\nQuestion: What is the main point of the article?\n\nAnswer:The use of contextualized tweets improves the accuracy of the model.\n\nQuestion: What is the main point of the article?\n\nAnswer:The use of contextualized tweets improves the accuracy of the model.\n\nQuestion: What is the main point of the article?\n\nAnswer:The use of contextualized", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "yes\n\n\n\nThe authors in BIBREF12 and BREF13 evaluate their learned representations on downstream tasks like sentiment analysis, hashtag prediction, and sentiment analysis. The authors in BREF14 and BREF15 evaluate their learned representations on downstream tasks like sentiment analysis, hashtag prediction, and sentiment analysis. The authors in BREF16 and BREF17 evaluate their learned representations on downstream tasks like sentiment analysis, hashtag prediction, and sentiment analysis. The authors in BREF18 and BREF19 evaluate their learned representations on downstream tasks like sentiment analysis", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "yes\n\n\n\"In this paper, we present a novel method to automatically detect demographic, linguistic, and psycholinguistic properties of people from their blogs. We use a dataset of 1,000,000 blogs, which we collected from the blogging platform Blogger.com. We first extract the text from the blogs and then use a set of 100 features to characterize the text. The features include the number of words, characters, and sentences, as well as the number of syllables, phonemes, and letters per word, the number of", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "3rd\n\n\nQuestion:What is the best performing model among the author's submissions?\n\nAnswer:3rd\n\nQuestion:What is the best performing model among the author's submissions?\n\nAnswer:3rd\n\nQuestion:What is the best performing model among the author's submissions?\n\nAnswer:3rd\n\nQuestion:What is the best performing model among the author's submissions?\n\nAnswer:3rd\n\nQuestion:What is the best performing model among the author's submissions?\n\nAnswer:3rd\n\nQuestion:", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "DTA12222\n\n\nQuestion:What is the gold standard for the task?\n\nAnswer:TA22\n\nQuestion:What is the metric used for the task?\n\nAnswer:TA22\n\nQuestion:How many teams participated in the shared task?\n\nAnswer:12\n\nQuestion:How many systems participated in the shared task?\n\nAnswer:12\n\nQuestion:What is the evaluation metric used in the shared task?\n\nAnswer:TA22\n\nQuestion:What is the gold standard for the shared task?\n\nAnswer:TA2", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "10\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K Nearestimplementing a system that can understand car salesperson's language and provide a list of cars that match the customer's preferences. (Document[109]):\"The system is able to understand the customer's language and provide a list of cars that match the customer's preferences . The system is able to understand the customer's language and provide a list of cars that match the customer's preferences . The system is able to understand the customer's language and provide a list of cars that match the customer's preferences . The system is able to understand the customer'", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "Extended middle part of the sentence (BIBR)\n\n\nQuestion: What is the new context representation?\n\nAnswer: Extended Middle (IBR)\n\nQuestion: What is the extended middle?\n\nAnswer: the middle part of the sentence (IBR)\n\nQuestion: What is the extended middle part of the sentence?\n\nAnswer: the middle part of the sentence (IBR)\n\nQuestion: What is the extended middle part of the sentence?\n\nAnswer: the middle part of the sentence (IBR)\n\nQuestion: What is the extended middle part of the sentence", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "multi-\n\n\n\nmulti-gran architecture design\n\nmulti-gran architecture\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n\nmulti-gran\n", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "over 450\n\n\nQuestion: What is the purpose of the paper?\n\nAnswer:5\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:5\n\nQuestion: What is the main challenge of the paper?\n\nAnswer:5\n\nQuestion: What is the main method of the paper?\n\nAnswer:5\n\nQuestion: What is the main result of the paper?\n\nAnswer:5\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer:5\n\nQuestion: What is the main result of the paper?\n\n", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "26\n\n\n\nQuestion: What is the size of the dataset used in the paper?\n\nAnswer:2000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "BIBMost recent work has focused on learning a tree-based model, which is a recursive neural network that takes a sentence as input and outputs a parse tree. The parse tree is a hierarchical representation of the sentence, where each node represents a word in the sentence and the edges represent the relationships between the nodes. The parse tree is then used to generate a sequence of words that can be used to generate a parse tree.\n\n\nAnswer:unanswerable\n\n\nQuestion:what is the state of the art?\n\nAnswer:unanswerable\n\nQuestion:what is the state of the art?", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The neutral word recognition of misspelleted words.\n\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to use a word recognition model to identify misspelled words.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to use a word recognition model to identify misspelled words.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to use a word recognition model to identify misspelled words.\n\nQuestion: What is", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "DSTACCESS 20.16.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEADVoccurve-1\n\n\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:A new model for generating descriptions of tables from the textual data.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:A new model for generating descriptions of tables from the textual data.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:A new model for generating descriptions of tables from the textual data.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:A new model for", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "AutoJudge1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "surface form (1) surface form (2) morphology (3) phonology (4) syntax (5) semantics (6) pragmatics (7) discourse (8) lexicon (9) semantics (10) pragmatics (11) discourse (12) lexicon (13) semantics (14) pragmatics (15) discourse (16) lexicon (17) semantics (18) pragmatics (19) discourse (20) lexicon (21) semantics (22) prag", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "the positive direction of the direction of the semantically related words (INLINE)\n\n\nQuestion: What is the name of the paper?\n\nAnswer: the name of the paper (INLINE)\n\nQuestion: What is the name of the paper?\n\nAnswer: the name of the paper (INLINE)\n\nQuestion: What is the name of the paper?\n\nAnswer: the name of the paper (INLINE)\n\nQuestion: What is the name of the paper?\n\nAnswer: the name of the paper (INLINE)\n\nQuestion: What is the name of the", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English (INLINE)\n\n\n\n\nQuestion: what is the domain of the data?\n\nAnswer:Europar (Europar)\n\n\nQuestion: what is the language of the data?\n\nAnswer:English (English)\n\nQuestion: what is the language of the data?\n\nAnswer:English (English)\n\nQuestion: what is the language of the data?\n\nAnswer:English (English)\n\nQuestion: what is the language of the data?\n\nAnswer:English (English)\n\nQuestion: what is the language of the data?\n\nAnswer", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "unanswerable\n\n\n\nTheir. language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "disinformation\n\n\ndisinformation\n\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin the family of text-based games called Coin Collector (BIBM) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) BIB) B", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINEFORM0\n\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: INLINE0\n", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "the ability of our model to capture the bias in the process of collecting or annotating data (BIBREF19)\n\n\nAnswer: The ability of our model to capture the bias in the process of collecting or annotating data (BREF19)\n\n\nAnswer: The ability of our model to capture the bias in the process of collecting or annotating data (BREF19)\n\n\nAnswer: The ability of our model to capture the bias in the process of collecting or annotating data (BREF19)\n\n\nAnswer: The ability of our model to capture", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "Conclusion\nWe would be driven by the fact that our model is more robust than the baseline model in terms of the number of generated descriptions that are correct.\n\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "\"In our proposed modification to the objective function of the cost function of any word in the vocabulary is modified by the addition of an additive term that is a function of the concept of the word and the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the concept of the", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "unanswerable\n\n\n\n\n\"We use the English Wikipedia corpus for our experiments. We use the English Wikipedia dump from 2014-011-01 and the corresponding Wikipedia dump from 2014-01-01. The dump contains all the articles and their revisions from 2014-01-01 to 2014-01-01. The dump is available at the following URL: http://dumps.wikimedia.org/enwiki/20140111111111", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "novel document encoding of document-level representations (Document-level representations)\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: novel document-level representations (Document-level representations)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: novel document-level representations (Document-level representations)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: novel document-level representations (Document-level representations)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: novel document-level representations (Document-level representations)\n", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The adaptively (Section SECREFIN)\n\n\nAnswer:adaptively (SEFIN)\n\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adaptively (SEFIN)\nAnswer:adapt", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "0.57373\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "22222,880000\n\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:22,800\n\nAnswer:", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "sentence (unanswerable)\n\n\n\nQuestion: What is the definition of instance?\n\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?\n\nAnswer:unanswerable\n\nQuestion: What is the definition of instance?", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "personal attacks (unanswerable)\n\n\n\nQuestion: What is the goal of the CRAFT model?\n\nAnswer: forecasting the future (unanswerable)\n\nQuestion: What is the goal of the CRAFT model?\n\nAnswer: forecasting the future (unanswerable)\n\nQuestion: What is the goal of the CRAFT model?\n\nAnswer: forecasting the future (unanswerable)\n\nQuestion: What is the goal of the CRAFT model?\n\nAnswer: forecasting the future (unanswerable)\n\nQuestion", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "We randomly sampledifferent datasets from China Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial Judicial", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "unanswerable\n\n\nThe model's output is compared to the reference style, which is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original input and a reference style sentence. The reference style is a concatenation of the original", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "18.2%\n\n\n\nQuestion: What is the best performing model?\n\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator\n\nQuestion: What is the best performing model?\nAnswer:Pointer-Generator", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "unanswered (Document[120]):\"The human judgments were assembled by a group of 50 native Tamil speakers . The judgments were obtained by asking the annotators to judge the sentences and giving the sentences and giving the judgments were obtained by asking the annotators to judge the sentences and giving the judgments were obtained by asking the annotators to judge the sentences and giving the judgments were obtained by asking the annotators to judge the sentences and giving the judgments were obtained by asking the annotators to judge the sentences and giving the judgments were obtained", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Joint model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "Ja-\n\n\nThe baseline is the best model we can obtain from the in-domain data.\n\n\nAnswer:\nunanswerable\n\nQuestion:\nwhat is the best model?\n\nAnswer:\nThe best model is the MT5 model.\n\nAnswer:\nyes\n\nQuestion:\nwhat is the best model?\n\nAnswer:\nThe best model is the MT5 model.\n\nAnswer:\nyes\n\nQuestion:\nwhat is the best model?\n\nAnswer:\nThe best model is the MT5 model.\n\nAnswer:\nyes\n", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "LW1. network density (D)\n2. diameter (D)\n3. diameter (D)\n4. diameter (D)\n5. diameter (D)\n6. diameter (D)\n7. diameter (D)\n8. diameter (D)\n9. diameter (D)\n10. diameter (D)\n11. diameter (D)\n12. diameter (D)\n13. diameter (D)\n14. diameter (D)\n15. diameter (D)\n16. diameter (D)\n17. diameter (D", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "ELMoEarXLM (Extended Layered Multi-task)\n\n\n\nQuestion:What is the best performing model?\n\nAnswer:ELMo (Extended Layered Multi-task)\n\nQuestion:What is the best performing model?\n\nAnswer:ELMo (Extended Layered Multi-task)\n\nQuestion:What is the best performing model?\n\nAnswer:ELMo (Extended Layered Multi-task)\n\nQuestion:What is the best performing model?\n\nAnswer:ELMo (Extended Layered Multi-task)\n\n", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "relation extraction (HR)\n\n\n\nQuestion:What is the core component of KBQA?\n\nAnswer:relation (HR)\n\nQuestion: What is the core component of KBQA?\nAnswer:relation (HR)\n\nQuestion: What is the core component of KBQA?\nAnswer:relation (HR)\n\nQuestion: What is the core component of KBQA?\nAnswer:relation (HR)\n\nQuestion: What is the core component of KBQA?\nAnswer:relation (HR)\n\nQuestion: What is the core component", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "BIBReward :::\n\n\nThe proposed method adapts an off-the-shelf reward learning algorithm from RL to the supervised learning setting for joint data manipulation and model training.\n\nAnswer:IBM :\n\nThe proposed method adapts an off-the-shelf reward learning algorithm from RL to the supervised learning setting for joint data manipulation and model training.\n\nAnswer:IBM :\n\nThe proposed method adapts an off-the-shelf reward learning algorithm from RL to the supervised learning setting for joint data manipulation and model training.\n", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "BioASQ: BioASQ\n\n\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioASQ\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioASQ\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioASQ\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioASQ\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioASQ\n\nQuestion:What is the BioASQ dataset?\n\nAnswer:BioAS", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "TACMost of the summaries in the TACC dataset are manually annotated by human annotators. The annotators are asked to evaluate the summaries based on the following criteria:\n\n1. Does the summary capture the main idea of the original article?\n2. Does the summary provide a good overview of the original article?\n3. Does the summary provide a good summary of the original article?\n4. Does the summary provide a good overview of the original article?\n5. Does the summary provide a good summary of the original article?\n6. Does the summary provide a good overview", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The word importance is calculated by the gradient of the input word embeddings with respect to the output embeddings. (SECREF)\n\nAnswer: The importance of a word is calculated by the gradient of the input word embeddings with respect to the output embeddings (SECREF)\n\nAnswer: The importance of a word is calculated by the gradient of the input word embeddings with respect to the output embeddings (SECREF)\n\nAnswer: The importance of a word is calculated by the gradient of the input word embeddings with respect to the output embeddings (SE", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "BIBMost of the state models include the following:\n\n\n1. BIBM (BIBM)\n2. BIBM (BIBM)\n3. BIBM (BIBM)\n4. BIBM (BIBM)\n5. BIBM (BIBM)\n6. BIBM (BIBM)\n7. BIBM (BIBM)\n8. BIBM (BIBM)\n9. BIBM (BIBM)\n10. BIBM (BIBM)\n11", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "significant improvement over strong (Document[1111]):\"We show improvement over strong baselines in both low data regime and class imbalance settings . The improvement is significant in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in both settings, and the improvement is strong in", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "multiple choice (MCQA)\n\n\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What is the main focus of the paper?\n\nAnswer:MCQ\n\nQuestion: What", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseem BERTbase (Waseem)\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:BERTbase (Waseem)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:BERTbase (Waseem)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:BERTbase (Waseem)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:BERTbase (Waseem)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "Transformer- RNN (RNN)\n- Transformer (Transformer)\n- BERT (Bidirectional Encoder Representations from Transformers)\n- RoBERT (RoBERT)\n- XLNet (XLNet)\n- DistilBERT (DistilBERT)\n- T5 (T5)\n- BART (BART)\n- XLM (XLM)\n- XLNet (XLNet)\n- RoBERT (RoBERT)\n- DistilBERT (DistilBERT)\n- T5 (", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "F1. We propose a new evaluation metric called F1-score, which is the harmonic mean of precision and recall.\n\n\nAnswer:1. We propose a new evaluation metric called F1-score, which is the harmonic mean of precision and recall.\n\n\nAnswer:1. We propose a new evaluation metric called F1-score, which is the harmonic mean of precision and recall.\n\nAnswer:1. We propose a new evaluation metric called F1-score, which is the harmonic mean of precision and recall.\n\nAnswer:1. We propose a new evaluation metric called", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "unanswerable (Document-level evaluation (§ SECREF001)\n\n\nAnswer:Document-level evaluation (§ SEF01)\n\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:Document-level evaluation (§ SEF01)\n\nAnswer:", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "2- 2-layers LSTM model (CE)\n- 2-layers LSTM (CE)\n- 2-layers LSTM (CE)\n- 3-layers LSTM (CE)\n- 4-layers LSTM (CE)\n- 5-layers LSTM (CE)\n- 6-layers LSTM (CE)\n- 7-layers LSTM (CE)\n- 8-layers LSTM (CE)\n- 9-layers LSTM (CE)\n- 10-", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "professional translation artifacts (Document[1]):\"The translation artifacts are introduced by the professional translation, which is the translation of the original text into another language . The artifacts are introduced by the professional translation, which is the translation of the original text into another language . The artifacts are introduced by the professional translation, which is the translation of the original text into another language . The artifacts are introduced by the professional translation, which is the translation of the original text into another language . The artifacts are introduced by the professional translation, which is the translation of the original text into another language . The artifacts are introduced by", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "end-to-to-to-end MRCNN\n\n\n\n\nAnswer:end-to-end MNN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end-to-end MN\n\n\nAnswer:end", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Acknowledgments\n\n\n\n\nQuestion: What is the main contribution of this paper?\n\n\nAnswer:The major contribution of this paper is to propose a novel model for sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sensing sens", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "Human-\n\n\nThe dataset is a large collection of text files, each containing a character's profile. Each file is in the format of a JSON object, with the following fields:\n\n* name: the character's name\n* profile: a brief description of the character\n* attributes: a list of attributes that describe the character\n* skills: a list of skills that the character has\n* strengths: a list of strengths that the character has\n* weaknesses: a list of weaknesses that the character has\n* personality: a list of personality traits\n* background", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "Reuters' Reuters' (Document[10]):\"Reuters\"'\n\n\nQuestion: What is the main contribution of this work?\n\nAnswer:The main contribution of this work is to propose a novel method to compare the performance of different methods in text classification tasks, and to propose a new method to compare the performance of different methods in text classification tasks, and to propose a new method to compare the performance of different methods in text classification tasks, and to propose a new method to compare the performance of different methods in text classification tasks, and to propose a new method to compare the performance of", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "Table TABSTUD\n\n\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:The main contribution of the paper is a novel approach to improve the performance of machine learning models by leveraging the disagreement between the crowd and the experts.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:The main contribution of the paper is a novel approach to improve the performance of machine learning models by leveraging the disagreement between the crowd and the experts.\n\nQuestion:What is the main contribution of the paper?\n\nAnswer:The main contribution of", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "By examining the results of recently published studies BIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIBIB", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "WebQuestions\n\n\n\n1. Simple relation extraction\n2. Question Answering\n3. Question Answering\n4. Question Answering\n5. Question Answering\n6. Question Answering\n7. Question Answering\n8. Question Answering\n9. Question Answering\n10. Question Answering\n11. Question Answering\n12. Question Answering\n13. Question Answering\n14. Question Answering\n15. Question Answering\n16. Question Answering\n17. Question Answering\n18. Question Answering\n19.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "spelling\n\nunanswerable\n\n\nQuestion: What is the main advantage of their approach?\n\nAnswer:\nunanswerable\n\nQuestion: What is the main disadvantage of their approach?\nAnswer:\nunanswerable\n\nQuestion: What is the main advantage of their approach?\nAnswer:\nunanswerable\n\nQuestion: What is the main disadvantage of their approach?\nAnswer:\nunanswerable\n\nQuestion: What is the main advantage of their approach?\nAnswer:\nunanswerable\n\nQuestion: What is the main disadvantage of their approach?", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "We crawl crawl crawl\n\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a large dataset of ironic tweets\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a large dataset of ironic tweets\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a large dataset of ironic tweets\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a large dataset of ironic tweets\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:a large", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The global context is the information about the whole document, while local context is the information about a specific sentence or a small part of the document. [1]\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: a novel approach to extracting the local and global context of a scientific paper, which is then used to generate a summary of the paper. [1]\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: a novel approach to extracting the local and global context of a scientific paper, which is then used to generate a summary of the paper. [1", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "Knowledge Base\n\n\nKBQA:\n\nA:\n\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA:\nA", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "They applied LDA: They utilized LDA and Gibbs to evaluate ISWC and WWW publications in computer science between 2013 and 2017.\n\n\nQuestion: What is the main goal of this paper?\n\nAnswer:The main goal of this paper is to present a taxonomy of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and applications of recommendation systems and", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "yes (unanswerable)\n\n\nQuestion: Do the authors believe that the gap between humans and machines is mainly due to the lack of general knowledge?\n\nAnswer:yes\n\nQuestion: Do the authors believe that the gap between humans and machines is mainly due to the lack of general knowledge?\n\nAnswer:yes\n\nQuestion: Do the authors believe that the gap between humans and machines is mainly due to the lack of general knowledge?\n\nAnswer:yes\n\nQuestion: Do the authors believe that the gap between humans and machines is mainly due to the lack of general knowledge?\n\nAnswer:yes\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "65555% (Document[29]):\"The gender imbalance is 555% (male / female) and the age imbalance is 55% (younger / older).\"\n\n\nQuestion: What is the proportion of women in the corpus?\n\nAnswer: 33% (Document[29]):\"The gender imbalance is 33% (male / female) and the age imbalance is 33% (younger / older).\"\n\n\nQuestion: What is the proportion of men in", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "unanswer not provided\n\n\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\nQuestion: What was the architecture of the baseline model?\n\nAnswer:not provided\n\n", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "unanswer returned by the top 5 systems (unanswerable)\n\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.61 (unanswerable)\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.61 (unanswerable)\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.61 (unanswerable)\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.61 (unanswerable)\n\nQuestion: What was their highest F1 score", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "BLENS\n\n\n\n1. Automatic evaluation\n2. Human evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "unanswerable\n\n\n\nThe system is based on a content-based approach, where the text of the blog post is used as input. The system uses a machine learning algorithm to classify the blog post into one of the 10 categories. The system is based on a content-based approach, where the text of the blog post is used as input. The system uses a machine learning algorithm to classify the blog post into one of the 10 categories.\n\n\nAnswer:\nThe system is based on a content-based approach, where the text of the blog post is used as input. The system", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Cyberlin/Polit/Polit (Bib)\n\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel approach to keyword extraction\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel approach to keyword extraction\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel approach to keyword extraction\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A novel approach to keyword extraction\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:A", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "We used several datasets including the SemEvaluated Question Answering Challenge (SQuAD) dataset, the Medical Information Mart (MIM) dataset, and the Medical Information Mart (MIM) dataset. (Document[10]):\"We used several datasets including the SemEval Question Answering Challenge (SQuAD) dataset, the Medical Information Mart (MIM) dataset, and the Medical Information Mart (MIM) dataset.\"\n\nQuestion: What is the effectiveness of the end-to-end approach?\n\nAnswer: outperformed the baseline methods (Document[10]):\"", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Conditional Copy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "the optimization challenge in our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes from the intractability of our neural network comes", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Logistic regression (BIBM)\n\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:A novel human-in-the-loop approach for event detection that leverages crowdsourcing and social media data to improve the performance of event detection models. (IB)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:A novel human-in-the-loop approach for event detection that leverages crowdsourcing and social media data to improve the performance of event detection models. (IB)\n\nQuestion: What is the main contribution of the paper", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "INLINEFORM0\n\n\n\nQuestion:what is the size of the real-world divorce dataset?\n\nAnswer:INLINEFORM0\n\nQuestion:what is the size of the real-world dataset?\n\nAnswer:INLINEFORM0\n\nQuestion:what is the size of the real-world dataset?\n\nAnswer:INLINEFORM0\n\nQuestion:what is the size of the real-world dataset?\n\nAnswer:INLINEFORM0\n\nQuestion:what is the size of the real-world dataset?\n\nAnswer:INLINEFORM0\n\nQuestion:what", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "MCScriptsnippets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "Back Translation\n\n\n\n1. Back Translation\n2. Back Translation\n3. Back Translation\n4. Back Translation\n5. Back Translation\n6. Back Translation\n7. Back Translation\n8. Back Translation\n9. Back Translation\n10. Back Translation\n11. Back Translation\n12. Back Translation\n13. Back Translation\n14. Back Translation\n15. Back Translation\n16. Back Translation\n17. Back Translation\n18. Back Translation\n19. Back", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "significant (ALO)\n\n\n\nAnswer: significant\n\nAnswer:significant\n\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer:significant\n\nAnswer", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The distinctive\n\n\n\nThe use of the vocabbreviation يان (ya n)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word أين (Ayn)\nThe use of the word ", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The authors show that the stronger generalization is achieved by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number of games (Appendix A.2) and by training on a larger number", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "To investigate gender bias in ASR\n\n\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: gender bias\n\nQuestion:", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "A predictive certain kinds of behavioral dogm\n\n\n\nQuestion: What is the corpus of annotated posts?\n\nAnswer: 5000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "yes\n\n\nThe dataset is available at <https://github.com/menglisheng/visual-qa>\n\n\n\nQuestion: What is the dataset used in the paper?\n\nAnswer:\nWikipedia\n\n\nQuestion: What is the dataset used in the paper?\n\nAnswer:\narXiv\n\n\nQuestion: What is the dataset used in the paper?\n\nAnswer:\nWikipedia\n\n\nQuestion: What is the dataset used in the paper?\n\nAnswer:\narXiv\n\n\nQuestion: What is the dataset used in the paper?\n\n", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "MPQ:MPQGensim200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "They propose to use the estimated word importance scores to detect the under-translated words. Specifically, we first calculate the word importance scores for each word in the source sentence. Then, we sort the words in the source sentence by their word importance scores. Finally, we select the top k words with the highest word importance scores as the under-translated words. (SECRET)\n\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The main conclusion of the paper is that the word importance scores can be used to detect the under-translated words. (SECRET)", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "Wikipedia (1) an expanded version of the Conversations Gone dataset BIBM (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2018) (2", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "yes (unanswerable)\n\n\n\nQuestion: did they use any pre-trained models?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained models?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained models?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained models?\n\nAnswer: (unanswerable)\n\nQuestion: did they use any pre-trained models?\n\nAnswer: (unanswerable)\n\nQuestion: did", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "Europarapartificially, we are interested in the following training data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "yes\n\n\"We tested our model on two datasets: FIGREF1 and FIGREF2 . FIGREF1 is a Chinese social media dataset, which is a subset of the FIGREF2 dataset. FIGREF1 is a Chinese social media dataset, which is a subset of the FIGREF2 dataset. FIGREF1 is a Chinese social media dataset, which is a subset of the FIGREF2 dataset.\"\n\n\nQuestion: What is the FIGREF1 dataset?\nAnswer:\n\"The FIGREF1 dataset is a Chinese social media dataset, which is a subset of", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "a matrix that shows the similarity between pairs of words in a corpus (Document[10]):\"A second - order co-occurrence matrix is a matrix that shows the similarity between pairs of words in a corpus . It is a generalization of the first - order co-occurrence matrix, where the similarity between two words is the number of times they co-occur in the corpus . The similarity between two words is the number of times they co-occur in the corpus . The similarity between two words is the number of times they co-occur in the corpus . The similarity between two", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "precision, recall, F10.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "unanswerable\n\n\nThe authors report results on English data only.\n\n\nAnswer:\nunanswerable\n\nQuestion: Do the authors report results on the effectiveness of the proposed approach?\n\nAnswer:\n\nThe authors report results on the effectiveness of the proposed approach.\n\nAnswer:\n\nunanswerable\n\nQuestion: Do the authors report results on the effectiveness of the proposed approach?\n\nAnswer:\n\nThe authors report results on the effectiveness of the proposed approach.\n\nAnswer:\n\nunanswerable\n\nQuestion: Do the authors report results on the effect", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "by their predicted difficulty level of difficulty (BIBM)\n\n\nAnswer: by their predicted difficulty (BM)\n\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their predicted difficulty (BM)\n\nAnswer: by their", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "approx\n\n\n\nMCScripture:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "yes (unanswerable\n\n\n\nQuestion: Do they use a copy mechanism?\n\nAnswer:yes\n\nQuestion: Do they use a gated mechanism?\n\nAnswer:yes\n\nQuestion: Do they use a bifold mechanism?\n\nAnswer:no\n\nQuestion: Do they use a gated mechanism?\n\nAnswer:yes\n\nQuestion: Do they use a bifold mechanism?\n\nAnswer:no\n\nQuestion: Do they use a gated mechanism?\n\nAnswer:yes\n\nQuestion: Do they use a bifold mechanism?\n\nAnswer:no", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "ROGold (ROU)\n\n\n\nQuestion:What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a new method for evaluating the quality of abstractive summaries.\n\nQuestion:What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a new method for evaluating the quality of abstractive summaries.\n\nQuestion:What is the main contribution of this paper?\n\nAnswer:The main contribution of this paper is to propose a new method for evaluating the quality of abstractive summaries", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "(1) Naive (1) Naive (2) mSDA (3) mSDA (4) ADAN (5) ADAN (6) Auxiliary (7) Auxiliary (8) Auxiliary (9) Auxiliary (10) Auxiliary (11) Auxiliary (12) Auxiliary (13) Auxiliary (14) Auxiliary (15) Auxiliary (16) Auxiliary (17) Auxiliary (18)", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "590.0%\n\n\n\nQuestion: What is the name of the paper?\n\nAnswer:Document Analysis and Summarization Using a Hybrid Approach\n\nQuestion: What is the name of the paper?\n\nAnswer:Document Analysis and Summarization Using a Hybrid Approach\n\nQuestion: What is the name of the paper?\n\nAnswer:Document Analysis and Summarization Using a Hybrid Approach\n\nQuestion: What is the name of the paper?\n\nAnswer:Document Analysis and Summarization Using a Hybrid Approach\n\nQuestion: What is the", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "word embeddifferent kinds of semantic embeddings (e.g., INLINEFORM0)\n\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:a method for quantifying the semantic similarity of words (INLINEFORM0)\n\nQuestion: What is the main contribution of the paper?\nAnswer:a method for quantifying the semantic similarity of words (INLINEFORM0)\n\nQuestion: What is the main contribution of the paper?\nAnswer:a method for quantifying the semantic similarity of words (INLINEFORM0)\n\nQuestion: What is the main contribution of", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "US mainstream of Twitter posts (twe used the Twitter API to collect tweets) (BIBM)\n\n\nAnswer:US mainstream media (B)\n\n\nAnswer:US mainstream media (B)\n\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer:US mainstream media (B)\n\nAnswer", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The political parties and their followers\n\n\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party\n\nAnswer: the political party", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "In their own human study (Table TABREF28)\n\n\n\nAnswer: unanswerable\n\n\nAnswer: unanswerable\n\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswer", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "F1. F1 - macro F1 (Section SECREFREF1)\n\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:. macro F1 (Section SECREF1)\n\nAnswer:.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "unanswerable\n\n\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n\nAnswer:\nYes\n", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "50\n\n\nThe Wikipedia dataset consists of 50K articles from the arXiv dataset, where each article is represented as a 10000-dimensional vector.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "unanswerable\n\n\n\nThe translation artifacts we have identified in this paper are not specific to the XLM model, but rather to the general use of machine translation in NLP tasks.\n\nThe artifacts we have identified in this paper are not specific to the XLM model, but rather to the general use of machine translation in NLP tasks.\n\nThe artifacts we have identified in this paper are not specific to the XLM model, but rather to the general use of machine translation in NLP tasks.\n\nThe artifacts we have identified in this paper are not specific to the XLM model,", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "Liu\n\n\n\nThe paper does not provide any metrics to establish that this makes chatbots more knowledgeable.\n\n\n\nAnswer:\nunanswerable\n\n\nAnswer:\n\nThe paper does not provide any metrics to establish that this makes chatbots more knowledgeable.\n\n\nAnswer:\n\nThe paper does not provide any metrics to establish that this makes chatbots more knowledgeable.\n\n\nAnswer:\n\nThe paper does not provide any metrics to establish that this makes chatbots more knowledgeable.\n\n\nAnswer:\n\nThe paper does not provide any metrics to", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Insight\n\n\n\n\n1. Topic\n2. Linguistic\n3. Computational\n4. Quantitative\n5. Qualitative\n6. Mixed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Reddit data from Twitter (Document[111]):\"We use data from Twitter to study hate speech . We use a dataset of 10 million tweets from 2010 to 2013, which we collected using the Twitter API . We use the tweet text to train our machine learning models . We use the tweet text to train our machine learning models . We use the tweet text to train our machine learning models . We use the tweet text to train our machine learning models . We use the tweet text to train our machine learning models . We use the tweet text to", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Energy (Energy)\n\n\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which stock sector achieved the best performance?\n\nAnswer:Energy\n\nQuestion: Which", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The authors have diverse backgrounds in the humanities (BIBM, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Sociology) (BIB, BIB, CS, Linguistics, and Soci", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "SVM\n\n\n\nWe experimented on the annotated data using various machine learning methods. We used the following methods:\n\n1. Naive Bayes classifier\n2. Naive Bayes classifier with smoothing\n3. Naive Bayes classifier with smoothing and feature selection\n4. Naive Bayes classifier with smoothing and feature selection and feature selection\n5. Naive Bayes classifier with smoothing and feature selection and feature selection\n6. Naive Bayes classifier with smoothing and feature selection and feature selection\n7. Naive Bay", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "yes\n\n\n\n\n\nQuestion: What is the main contribution of this paper?\n\n\nAnswer:\n\nAnswer:\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer:\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer:\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer:\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer:\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "Conversatile dialogue to the state of the art on conversational systems, in three perspectives: types of interactions, types of systems, and types of reasoning.\n\n\nAnswer:unanswerable\n\nQuestion: What is the state of the art on conversational systems?\n\nAnswer:unanswerable\n\nQuestion: What is the state of the art on conversational systems?\n\nAnswer:unanswerable\n\nQuestion: What is the state of the art on conversational systems?\n\nAnswer:unanswerable\n\nQuestion: What is the state of the art on conversational systems?", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "a new representation of state-level data on violent conflict (BIBP) (BIBP)\n\n\nAnswer: a new dataset of state-level data on violent conflict (BP) (BP)\n\n\nAnswer: a new dataset of state-level data on violent conflict (BP) (BP)\n\n\nAnswer: a new dataset of state-level data on violent conflict (BP) (BP)\n\nAnswer: a new dataset of state-level data on violent conflict (BP) (BP)\n\nAnswer: a new dataset of state-level data", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Doc2. \n\n\n\n\n1. Bag of Words\n2. Doc2Vec\n3. Doc2Vec\n4. Doc2Vec\n5. Doc2Vec\n6. Doc2Vec\n7. Doc2Vec\n8. Doc2Vec\n9. Doc2Vec\n10. Doc2Vec\n11. Doc2Vec\n12. Doc2Vec\n13. Doc2Vec\n14. Doc2Vec\n15. Doc2Vec\n16. Doc2Vec\n17. Doc2Vec\n18. Doc2Vec\n1", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "argument components (1) Claimplementing the task of identifying argument components (IN)\n\n\nAnswer:argument components (IN)\n\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN)\n\nAnswer:argument components (IN", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "yes (unanswerable)\n\n\n\nQuestion: Do they report results on other languages?\n\n\nAnswer:no (unanswerable)\n\nQuestion: Do they report results on other languages?\n\nAnswer:no (unanswerable)\n\nQuestion: Do they report results on other languages?\n\nAnswer:no (unanswerable)\n\nQuestion: Do they report results on other languages?\n\nAnswer:no (unanswerable)\n\nQuestion: Do they report results on other languages?\n\nAnswer:no (unanswerable)\n\nQuestion: Do they report results on", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "unanswerable\n\n\n\nunanswerable\n\n\nQuestion:What is the main contribution of this paper?\n\nAnswer:\n\nAnswer\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:\n\nAnswer\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "unanswerable\n\n\nThe lexiconic words are not only a subset of dehumanizing words, but also include words that are not necessarily derogatory but are used to dehumanize a person. For example, we use the word \"dehuman\" is a subset of the word \"dehuman\" is a subset of the word \"human\" is a subset of the word \"human\" is a subset of the word \"human\" is a subset of the word \"human\" is a subset of the word \"human\" is a subset of the word \"human\" is a subset of the word \"human\" is", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "a state\n\n\n\n\nA node in a network is a representation of a state or an entity, such as a country, a person, or a corporation, that is connected to other nodes by one or more edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "We create a new corpus which is, which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation field to date (Document[119]):\"We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation field to date (Document[119]):\"We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation field to date (Document[19]):\"We create a", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "We used the New York Times's own API to retrieve all articles published in the New York Times from 1981 to 2019, which we then filtered to only include articles that contain the word LGBTQ in their title.\n\n\nQuestion:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "yes\n\n\n\n\"In describing the research process, we have tried to highlight the importance of interdisciplinary insights. We have also tried to provide a set of best practices for working with interdisciplinary teams. We have also tried to provide a set of best practices for working with interdisciplinary teams. We have also tried to provide a set of best practices for working with interdisciplinary teams. We have also tried to provide a set of best practices for working with interdisciplinary teams. We have also tried to provide a set of best practices for working with interdisciplinary teams. We", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "thornate\n\n\n\n\nQuestion: What are the goals of the article?\n\nAnswer:\n\n\nQuestion: What are the three primary goals of the article?\n\nAnswer:\n\nQuestion: What are the three primary goals of the article?\n\nAnswer:\n\nQuestion: What are the three primary goals of the article?\n\nAnswer:\n\nQuestion: What are the three primary goals of the article?\n\nAnswer:\n\nQuestion: What are the three primary goals of the article?\n\nAnswer:\n\nQuestion: What are the three primary goals of the article?\n", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "accuracy (Table UIDREF!\n\n\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What is the accuracy of the system?\n\nAnswer:IDREF!\n\nQuestion:What", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "yes\n\n\"UNIT is a common assumption that the more a state is represented in the UNITS, the more it will reflect the preferences of its citizens. However, the relationship between the number of votes and the preferences of the citizens is not straightforward. The relationship between the number of votes and the preferences of the citizens is not straightforward. The relationship between the number of votes and the preferences of the citizens is not straightforward. The relationship between the number of votes and the preferences of the citizens is not straightforward. The relationship between the number of votes and the preferences of the citizens is not straightforward. The", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "\"First, the scope of the annotated data is limited to a single domain, which is the domain of the argumentation corpus BIBREF10 . The annotated data is not available for other domains, which is a problem for the annotated data is not available for other domains, which is a problem for the annotated data is not available for other domains, which is a problem for the annotated data is not available for other domains, which is a problem for the annotated data is not available for other domains, which is a problem for the annotated data is not available for other domains, which is a problem", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "manually annotated (Document[10]\n\n\n\nQuestion: How many languages are included in the dataset?\n\nAnswer: 12 (Document[1]):\"The MultiSim dataset consists of 12 languages, including English, French, German, Spanish, Italian, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi . The dataset is annotated with the following semantic relations: coreference, entailment, contradiction, and non-coreference.\"\n\nQuestion: What is the main focus of the paper?\n\nAnswer: semantic relations (Document[1]):", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "figurative (1) figurative language (e.g. \"the\", \"a\", \"an\") (2) argumentative language (e.g. \"the\", \"a\", \"an\") (3) figurative language (e.g. \"the\", \"a\", \"an\") (4) argumentative language (e.g. \"the\", \"a\", \"an\") (5) figurative language (e.g. \"the\", \"a\", \"an\") (6) argumentative language (e.g. \"the\", \"a\", \"an\") (7) figurative language", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "Welshanglais (Welsh)\n\n\nQuestion:What is the focus of the article?\n\nAnswer:The article introduces MultiSim, a new framework for multilingual semantic similarity estimation.\n\nQuestion: What is the main contribution of the article?\n\nAnswer:The main contribution of the article is a new framework for multilingual semantic similarity estimation.\n\nQuestion: What is the focus of the article?\n\nAnswer:The focus of the article is a new framework for multilingual semantic similarity estimation.\n\nQuestion: What is the main contribution of the article?\n\nAnswer:", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
